# -*- coding: utf-8 -*-
"""new_dataset_base.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nujXKykJ2pa3sohrQTbiLnJOxj31bm9w

# Read In Dataset
"""

!pip install transformers

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

import transformers
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification,AutoModelForTokenClassification,AutoTokenizer
import time
import spacy
import transformers
from transformers import BertForTokenClassification, AdamW
from sklearn.metrics import confusion_matrix, auc,roc_curve
from sklearn.preprocessing import MultiLabelBinarizer


transformers.__version__

from tqdm import tqdm, trange
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizerFast, BertConfig

from keras_preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import statistics as st
torch.__version__

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/new_dataset_clean_with5+.csv', encoding='UTF-8', na_filter= False, index_col=0)
def remove_non_ascii(text): 
    return ''.join(i for i in text if ord(i)<128) 
 
data['token'] = data['token'].apply(remove_non_ascii)

l = ['B-HPI_pos','B-HPI_neg','I-HPI_pos','I-HPI_neg','B-Descriptor']

data_d = data[data['tag'].isin(l)]

# Groupby and collect columns
data_group = data_d.groupby(
['file_sent'],as_index=False
)['text','tag'].agg(lambda x: list(x))
# Visualise data
l = []
for i,r in data_group.iterrows():
  for t in r['tag']
  if r['tag'][i]

#extend dictionary with different cases
dictionary = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/ultimate_dictionary_final7.csv')
#dictionary = pd.read_csv('/content/drive/MyDrive/MSc BERT/ultimate_dictionary_final7.csv')
markers_names =list(set(dictionary['use']))
markers_names += list(set(dictionary['Synonym']))
markers_names+=(list(set(dictionary['Synonym'].str.upper())))
markers_names+=(list(set(dictionary['use'].str.upper())))
markers_names+=(list(set(dictionary['Synonym'].str.lower())))
markers_names+=(list(set(dictionary['use'].str.lower())))
markers_names+=(list(set(dictionary['Synonym'].str.capitalize())))
markers_names+=(list(set(dictionary['use'].str.capitalize())))

#remove unlabeled markers
markers_string = ' '.join(markers_names)
import string
print(string.punctuation)
marker_names = data['token'][data['tag'].str.startswith('B-HPI')].unique()#[(data['tag'].str.startswith('B-HPI'))&(data['tag'].str.startswith('I-HPI'))].unique()
ner_names = data['token'][data['tag'].str.startswith('B')].unique()
from tensorflow.python.ops.gen_functional_ops import If

#account for markers that were not labeled as HPI/IHC etc
no_label = []
for i,r in data.iterrows():
  if r['token'].isascii() == False:
    print(r['token'])
    data.drop(i, axis=0, inplace=True)
  
  if r['token'] in marker_names and r['tag'] == 'O' and r['token'] not in string.punctuation and r['token'].isascii() and r['token'] in markers_string:
 
    
    data.drop(i, axis=0, inplace=True)
    if r['token']!='A' and r['token']!='c' and r['token']!='C' and r['token']!='E':
      no_label.append([r['token'],r['file_sent']])
  elif r['token'] in marker_names and r['tag'] == 'O' and r['token'] not in string.punctuation and r['token'].isascii() and r['token'] in markers_names:
    no_label.append([r['token'],r['file_sent']])
    data.drop(i, axis=0, inplace=True)

len(no_label)

data = data.rename(columns = {'token':'Word', 'tag':'Tag'})

#label annotations that have less than 500 instances with O tag or remove them
tag_ls = ['B-HPI_pos', 'I-HPI_pos',
          'B-HPI_neg','I-HPI_neg',
          'B-HPI_irrelevant', 'I-HPI_irrelevant',
          'B-HPI_equivocal', 
          'B-HPI_qualified','I-HPI_qualified',
          'B-Body Part, Organ, or Organ Component_pos','I-Body Part, Organ, or Organ Component_pos',
          'B-Neoplastic Process',
          'I-Neoplastic Process',
          'B-Descriptor',
          'I-Descriptor']
#uncomment to remove other markers instead of labeling 'O'
#tag_ls.append('O')
#data = data.loc[data['Tag'].isin(tag_ls)]          
data.loc[~data["Tag"].isin(tag_ls), 'Tag'] = 'O'
tag_ls.append('O')
no_o = tag_ls[:-1]

print("Number of tags: {}".format(len(data.Tag.unique())))
frequencies = data.Tag.value_counts()
print(frequencies)
tags = {}
for tag, count in zip(frequencies.index, frequencies):
    if tag != "O":
        if tag not in tags.keys():
            tags[tag] = count
        else:
            tags[tag] += count
    continue

print(sorted(tags.items(), key=lambda x: x[1], reverse=True))

#plot class distributions
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(color_codes=True)
sns.set(font_scale=1)
plt.figure(figsize=(15, 5))
#ax = sns.countplot('Tag', data= data, order=data['Tag'].value_counts().index)
ax = sns.countplot('Tag', data= data.loc[data['Tag'] != 'O'], order=data['Tag'].value_counts().index)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha="center")
plt.figure(figsize = ( 5 , 3 ))
plt.tight_layout()
plt.show()

data['text'].replace({'':np.nan}, inplace = True)
all = data[data['Tag'].isin(['B-HPI_pos','I-HPI_pos','B-HPI_neg','I-HPI_neg'])].reset_index(drop = True)
pos = data[data['Tag'].isin(['B-HPI_pos','I-HPI_pos'])].reset_index(drop = True)
neg = data[data['Tag'].isin(['B-HPI_neg','I-HPI_neg'])].reset_index(drop = True)
des = data[data['Tag'].isin(['B-Descriptor','I-Descriptor'])].reset_index(drop = True)
all_vc = pd.DataFrame(all['text'].value_counts().reset_index().values,columns =['text','Count'])
pos_valc = pd.DataFrame(pos['text'].value_counts().reset_index().values,columns =['POS_text','POS_Count'])
neg_valc = pd.DataFrame(neg['text'].value_counts().reset_index().values,columns =['NEG_text','NEG_Count'])
des_valc = pd.DataFrame(des['text'].value_counts().reset_index().values,columns =['DESCRIPTOR','DESCRIPTOR_Count'])
all_valc= all_vc.append(pos_valc)
all_valc = all_valc.append(neg_valc)
all_valc = all_valc.append(des_valc)
all_valc.to_csv('marker_counts.csv')

pos_valc = pd.DataFrame(pos['text'].value_counts())
pos_valc

neg_valc = pd.DataFrame(neg['text'].value_counts())
neg_valc

#convert tags to numerical vectors
labels_to_ids = {k: v for v, k in enumerate(data.Tag.unique())}
ids_to_labels = {v: k for v, k in enumerate(data.Tag.unique())}
print(labels_to_ids)
print(ids_to_labels)

class SentenceGetter(object):

    def __init__(self, data):
        self.n_sent = 1
        self.data = data
        self.empty = False
        agg_func = lambda s: [(w, t) for w, t in zip(s["Word"].values.tolist(),
                                                           s["Tag"].values.tolist())]
        self.grouped = self.data.groupby("file_sent").apply(agg_func)
        self.sentences = [s for s in self.grouped]

    def get_next(self):
        try:
            s = self.grouped["Sentence: {}".format(self.n_sent)]
            self.n_sent += 1
            return s
        except:
            return None

getter = SentenceGetter(data)

#visualise words/tags to index mappings
words = list(set(data['Word'].values))
n_words = len(words)
word2idx = {w:i for i,w in enumerate(words)}

tag_values = list(set(data["Tag"].values))
tag_values.append("PAD")
tag2idx = {t: i for i, t in enumerate(tag_values)}


data['Word_idx'] = data['Word'].map(word2idx)
data['Tag_idx'] = data['Tag'].map(tag2idx)
data.head()

len(data['specimen'].unique())

# Groupby and collect columns
data_group = data.groupby(
['file_sent'],as_index=False
)['Word', 'Tag', 'Word_idx', 'Tag_idx','text','file','type'].agg(lambda x: list(x))
# Visualise data
data_group

data_group['Report'] = data_group['Word'].str.join(" ")

#get duplicated sentences
dups = list(data_group['file_sent'][data_group['Report'].duplicated()])
data[~data['file_sent'].isin(dups)]

#save group data
data_group.to_csv("/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/data_group_hpi.csv")

sentences = [[word[0] for word in sentence] for sentence in getter.sentences]
print(sentences[0])
#sentences[2][36]

labels = [[lab[1] for lab in sentence] for sentence in getter.sentences]
print(labels[2])
#labels[2][36]

tag_values = list(set(data["Tag"].values))
tag_values.append("PAD")
tag2idx = {t: i for i, t in enumerate(tag_values)}

max_len = 512
bs = 16

"""SELECT GPU IF AVAILABLE"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
n_gpu = torch.cuda.device_count()
print(n_gpu)
print(device)

torch.cuda.get_device_name(0)

"""# BASE BERT

##Split Train/Test
"""

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

def tokenize_and_preserve_labels(sentence, text_labels):
    tokenized_sentence = []
    labels = []

    for word, label in zip(sentence, text_labels):

        # Tokenize the word and count # of subwords the word is broken into
        tokenized_word = tokenizer.tokenize(word)
        n_subwords = len(tokenized_word)

        # Add the tokenized word to the final tokenized word list
        tokenized_sentence.extend(tokenized_word)

        # Add the same label to the new list of labels `n_subwords` times
        labels.extend([label] * n_subwords)

    return tokenized_sentence, labels

tokenized_texts_and_labels = [
    tokenize_and_preserve_labels(sent, labs)
    for sent, labs in zip(sentences, labels)
]

tokenized_texts_and_labels[0]

tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]
labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]


print(tokenized_texts)
print(labels)

data.Tag.unique()

#padding for sentences with maximum length 200
input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],
                          maxlen=max_len, dtype="long", value=0.0,
                          truncating="post", padding="post")
input_ids[2]

#padding for labels
tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],
                     maxlen=max_len, value=tag2idx["PAD"], padding="post",
                     dtype="long", truncating="post")


#tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],
 #                    maxlen=max_len, value=tag2idx["O"], padding="post",
  #                   dtype="long", truncating="post")

"""Loading data for training and testing.Creating the mask to ignore the padded elements in the sequences.

"""

#split train/val + test inputs
X_tr_val_inputs, X_test_inputs, y_tr_val_tags, y_test_tags = train_test_split(input_ids, tags,
                                                            random_state=9, test_size=0.2)
#split train + val 
X_tr_inputs, X_val_inputs, y_tr_tags, y_val_tags = train_test_split(X_tr_val_inputs, y_tr_val_tags,
                                                            random_state=9, test_size=0.1)
#split tr/val + test masks
attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]
tr_val_masks, test_masks, _, _ = train_test_split(attention_masks, input_ids,
                                             random_state=9, test_size=0.2)
#split tr + val masks
split_at_mask = [[float(i != 0.0) for i in ii] for ii in tr_val_masks]
tr_masks, val_masks, _, _ = train_test_split(split_at_mask, tr_val_masks,
                                             random_state=9, test_size=0.1)

#split into 3 train/val/test 
#split 80 train, and 20 test. Split 80 into 90 train and 10 val
#after each epoch select best epoch from validation and use the best for testing

tr_inputs = torch.tensor(X_tr_inputs)
val_inputs = torch.tensor(X_val_inputs)
test_inputs = torch.tensor(X_test_inputs)

tr_tags = torch.tensor(y_tr_tags)
val_tags = torch.tensor(y_val_tags)
test_tags = torch.tensor(y_test_tags)

tr_masks = torch.tensor(tr_masks)
val_masks = torch.tensor(val_masks)
test_masks = torch.tensor(test_masks)

# wrap tensors
train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)
valid_data = TensorDataset(val_inputs, val_masks, val_tags)
test_data = TensorDataset(test_inputs, test_masks, test_tags)

# samplers for sampling the data during training, validation and testing
sampler = RandomSampler(train_data)
val_sampler = RandomSampler(valid_data)
test_sampler = SequentialSampler(test_data)

# dataLoader for train, validation and test sets
training_data_loader = DataLoader(train_data, sampler=sampler, batch_size=bs)
validation_data_loader = DataLoader(valid_data, sampler=val_sampler, batch_size=bs)
test_data_loader = DataLoader(test_data, sampler=test_sampler)

"""##Train

BertForTokenClassification is a 
fine-tuning model that wraps BertModel and adds token-level classifier on top of the BertModel. The token-level classifier is a linear layer that takes as input the last hidden state of the sequence
"""

import transformers
from transformers import BertForTokenClassification, AdamW

transformers.__version__

len(tag2idx)

model = BertForTokenClassification.from_pretrained(
    #"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
    "bert-base-uncased" ,
    #"emilyalsentzer/Bio_ClinicalBERT",
    #"dmis-lab/biobert-base-cased-v1.2",
    num_labels=len(tag2idx),
    output_attentions = False,
    output_hidden_states = False
)
model.cuda();

"""Select GPU if available"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

from transformers import AdamW
FULL_FINETUNING = True
if FULL_FINETUNING:
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'gamma', 'beta']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.0}
    ]
else:
    param_optimizer = list(model.classifier.named_parameters())
    optimizer_grouped_parameters = [{"params": [p for n, p in param_optimizer]}]

optimizer = AdamW(
    optimizer_grouped_parameters,
    lr=3e-5,
    eps=1e-8
)

from transformers import get_linear_schedule_with_warmup

epochs = 10 #100,200
max_grad_norm = 1.0

# Total number of training steps is number of batches * number of epochs.
total_steps = len(training_data_loader) * epochs

# Create the learning rate scheduler.
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

no_o = tag_ls[:-1]

no_o

"""labels = [  'B-HPI_pos', 'B-HPI_neg', 'I-HPI_neg', 'B-HPI_qualified',
       'B-Descriptor', 'I-Descriptor', 'B-HPI_irrelevant', 'I-HPI_pos',
       'I-HPI_qualified', 'I-HPI_irrelevant', 'B-Molecular_neg',
       'B-Molecular_pos', '', 'I-Molecular_pos', 'I-Molecular_neg',
       'B-Body Part, Organ, or Organ Component_pos',
       'I-Iody Part, Organ, or Organ Component_pos',
       'B-Neoplastic Process', 'I-Neoplastic Process', 'B-Modifier',
       'I-Modifier', 'B-Neoplastic Process_pos',
       'I-Neoplastic Process_pos', 'B-Diagnostic Procedure_pos',
       'I-Diagnostic Procedure_pos']

Macro Average
"""

## Store the average loss after each epoch so we can plot them.

import copy
loss_values, validation_loss_values, f1_val = [], [],[]
from sklearn.metrics import f1_score, classification_report
best_model_state = None
best_score = 1
for _ in trange(epochs, desc="Epoch"):
    # ========================================
    #               Training
    # ========================================
    # Perform one full pass over the training set.

    # Put the model into training mode.
    model.train()


    # Reset the total loss for this epoch.
    total_loss = 0

    # Training loop
    for step, batch in enumerate(training_data_loader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        # Always clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()
        # forward pass
        # This will return the loss (rather than the model output)
        # because we have provided the `labels`.
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask, labels=b_labels)
        # get the loss
        loss = outputs[0]
        # Perform a backward pass to calculate the gradients.
        loss.backward()
        # track train loss
        total_loss += loss.item()
        # Clip the norm of the gradient
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        # Update the learning rate.
        scheduler.step()
        

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(training_data_loader)
    print("Average train loss: {}".format(avg_train_loss))

    

    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)


    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    # Put the model into evaluation mode
    model.eval()
    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in validation_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)

    eval_loss = eval_loss / len(validation_data_loader) 
    validation_loss_values.append(eval_loss)
    print("Validation loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]
   
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]#[PAD]                             
    valid_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]#[PAD]
    f1_val.append(f1_score(pred_tags, valid_tags, average = 'macro',labels = tag_ls))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    try:
      print("Validation Accuracy: {}".format(accuracy_score(pred_tags, valid_tags)))
      #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
      #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
      print("Validation macro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'macro', labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
      print('Classification Report: {}'.format(classification_report(pred_tags, valid_tags,labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    

     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=tag_ls, zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=['B-HPI_pos','B-HPI_neg'], zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',

      print()

      print()
      mistakes = []
      predicted_tags = []
      true_tags = []
      for p, v in zip(pred_tags, valid_tags):
        predicted_tags.append(p)
        true_tags.append(v)
        if p != v:
          mistakes.append(p)
          mistakes.append(v)
          
      #print(f'predicted tags:{pred_tags}')
      #print(f'true tags:{valid_tags}')
      #print(f"mistakes list:{mistakes}")
      print(f'number of mistakes:{int(len(mistakes)/2)}')
      print('\n')

      if eval_loss < best_score and eval_loss > avg_train_loss:
          best_score = eval_loss
          best_model_state = copy.deepcopy(model.state_dict())
          print('current best evaluation loss is {0:.3f}'.format(best_score))
          print('\n')
    except:
      continue
print(f"final lowest evaluation loss is {best_score}")#excluding O class
    


 #if token is predicted incorectly, print token and surrounding tokens

## Store the average loss after each epoch so we can plot them.

'''import copy
loss_values, validation_loss_values, f1_val = [], [],[]
from sklearn.metrics import f1_score, classification_report
best_model_state = None
best_score = -1
for _ in trange(epochs, desc="Epoch"):
    # ========================================
    #               Training
    # ========================================
    # Perform one full pass over the training set.

    # Put the model into training mode.
    model.train()


    # Reset the total loss for this epoch.
    total_loss = 0

    # Training loop
    for step, batch in enumerate(training_data_loader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        # Always clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()
        # forward pass
        # This will return the loss (rather than the model output)
        # because we have provided the `labels`.
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask, labels=b_labels)
        # get the loss
        loss = outputs[0]
        # Perform a backward pass to calculate the gradients.
        loss.backward()
        # track train loss
        total_loss += loss.item()
        # Clip the norm of the gradient
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        # Update the learning rate.
        scheduler.step()
        

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(training_data_loader)
    print("Average train loss: {}".format(avg_train_loss))

    

    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)


    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    # Put the model into evaluation mode
    model.eval()
    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in validation_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)

    eval_loss = eval_loss / len(validation_data_loader) 
    validation_loss_values.append(eval_loss)
    print("Validation loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]
   
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]#[PAD]                             
    valid_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]#[PAD]
    f1_val.append(f1_score(pred_tags, valid_tags, average = 'macro',labels = no_o))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    try:
      print("Validation Accuracy: {}".format(accuracy_score(pred_tags, valid_tags)))
      #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
      #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
      print("Validation macro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'macro', labels=no_o, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
      print('Classification Report: {}'.format(classification_report(pred_tags, valid_tags,labels=no_o, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    

      ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=no_o, zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      print()

      print()
      mistakes = []
      predicted_tags = []
      true_tags = []
      for p, v in zip(pred_tags, valid_tags):
        predicted_tags.append(p)
        true_tags.append(v)
        if p != v:
          mistakes.append(p)
          mistakes.append(v)
          
      #print(f'predicted tags:{pred_tags}')
      #print(f'true tags:{valid_tags}')
      #print(f"mistakes list:{mistakes}")
      print(f'number of mistakes:{int(len(mistakes)/2)}')
      print('\n')

      if best_score < ret:
          best_score = ret
          best_model_state = copy.deepcopy(model.state_dict())
          print('current best score is {0:.3f}'.format(best_score))
          print('\n')
    except:
      continue
print(f"final best F1 macro score is {best_score}")#excluding O class
    


 #if token is predicted incorectly, print token and surrounding tokens   
    '''

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

import seaborn as sns

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(loss_values, 'b-o', label="training loss")
plt.plot(validation_loss_values, 'r-o', label="validation loss")

# Label the plot.
plt.title("Learning curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.savefig('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/base_loss_evloss.png')
plt.show()

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(f1_val, 'g-o', label="validation f1")
# Label the plot.
plt.title("Learning curve")
plt.xlabel("Epoch")
plt.ylabel("F1 Weigthed Average Score")
plt.legend()
plt.savefig('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/base_f_score_evloss.png')
plt.show()

"""Save the model"""

#add timestamp to file name to track the version of weights saved
t = time.localtime()
timestamp = time.strftime('%d_%b_%H_%M', t)
model_file = ('base_best_model_evloss'+timestamp+'.pt')
#path = f"./{model_file}"
path =f"/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/{model_file}"
#torch.save(model.state_dict(), path)
torch.save(best_model_state, path)

print(timestamp)

"""##Evaluate"""

model_file = ('base_best_model_evloss'+timestamp+'.pt')
path =f"/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/{model_file}"

best_model = BertForTokenClassification.from_pretrained(
    #"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
    #"emilyalsentzer/Bio_ClinicalBERT",
    "bert-base-uncased",
    #"dmis-lab/biobert-base-cased-v1.2",
    num_labels=len(tag2idx),
    output_attentions = False,
    output_hidden_states = False
)
best_model.to(device)
#best_model.load_state_dict(best_model_state)
best_model.load_state_dict(torch.load(path))

from sklearn.metrics import f1_score, classification_report
    loss_values, test_loss_values = [], []
    best_model.eval()
    # Reset the validation loss for this epoch.

    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels, tokens = [], [],[]
    for batch in test_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)

        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = best_model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        token_list = tokenizer.convert_ids_to_tokens(b_input_ids.to('cpu').numpy()[0])

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)
        tokens.extend(token_list)

    eval_loss = eval_loss / len(test_data_loader) 
    test_loss_values.append(eval_loss)


    print("Test loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]

    print(len(predictions))
    print(len(true_labels))
    print(predictions[0])
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[p_i] != "PAD"] 
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]                            
    test_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]
    print(len(pred_tags))
    print(len(test_tags))                              
    print("Test Accuracy: {}".format(accuracy_score(pred_tags, test_tags)))
    #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
    #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
    print("Test Macro F1-Score: {}".format(f1_score(pred_tags, test_tags, average = 'macro', labels =no_o)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    
    
    #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
    print('Classification Report: {}'.format(classification_report(pred_tags, test_tags, labels = no_o)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    #ret = (f1_score(pred_tags, test_tags, average = 'macro'))
    print()

    print()
    mistakes = []
    predicted_tags = []
    true_tags = []
    token_mistakes =[]
    
    #print(len(tokens))
    
    pred_tags = [tag_values[p_i] for p in predictions
                                  for p_i in p if tag_values[p_i] != "[PAD]"]   #[PAD]                          
    test_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"] #[PAD]
    for idx, (p, v) in enumerate(zip(pred_tags, test_tags)):
      predicted_tags.append(p)
      true_tags.append(v)
      if p != v :#and p != 'PAD':

        mistakes.append(p)
        mistakes.append(v)
        mistakes.append(tokens[idx-1])
        mistakes.append(tokens[idx])
        mistakes.append(tokens[idx+1])
        token_mistakes.append(tokens[idx])
    #print(f'predicted tags:{pred_tags}')
    #print(f'true tags:{valid_tags}')
    print(f"(predicted, true, before_mistake, token, after_mistake):{mistakes}")
    print(f'number of mistakes:{int(len(mistakes)/3)}')

    print(f'List of tokens predicted incorrectly:{token_mistakes}')

"""# IHC-BASE BERT

##Split Train/Test
"""

#tokenizer = BertTokenizerFast.from_pretrained("microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract", do_lower_case=False)
#tokenizer = BertTokenizerFast.from_pretrained("bert-base-cased", do_lower_case=False)

tokenizer = AutoTokenizer.from_pretrained("/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/IHC_BERT/weights/checkpoint-380000",do_lower_case=True)
#tokenizer = BertTokenizerFast.from_pretrained("emilyalsentzer/Bio_ClinicalBERT", do_lower_case=False)
#tokenizer = BertTokenizerFast.from_pretrained( "dmis-lab/biobert-base-cased-v1.2", do_lower_case=True)
#tokenizer = BertTokenizerFast.from_pretrained( "dmis-lab/biobert-base-cased-v1.2", do_lower_case=False)

def tokenize_and_preserve_labels(sentence, text_labels):
    tokenized_sentence = []
    labels = []

    for word, label in zip(sentence, text_labels):

        # Tokenize the word and count # of subwords the word is broken into
        tokenized_word = tokenizer.tokenize(word)
        n_subwords = len(tokenized_word)

        # Add the tokenized word to the final tokenized word list
        tokenized_sentence.extend(tokenized_word)

        # Add the same label to the new list of labels `n_subwords` times
        labels.extend([label] * n_subwords)

    return tokenized_sentence, labels

tokenized_texts_and_labels = [
    tokenize_and_preserve_labels(sent, labs)
    for sent, labs in zip(sentences, labels)
]

tokenized_texts_and_labels[0]

tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]
labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]


print(tokenized_texts)
print(labels)

data.Tag.unique()

#padding for sentences with maximum length 200
input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],
                          maxlen=max_len, dtype="long", value=0.0,
                          truncating="post", padding="post")
input_ids[2]

#padding for labels
tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],
                     maxlen=max_len, value=tag2idx["PAD"], padding="post",
                     dtype="long", truncating="post")


#tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],
 #                    maxlen=max_len, value=tag2idx["O"], padding="post",
  #                   dtype="long", truncating="post")

"""Loading data for training and testing.Creating the mask to ignore the padded elements in the sequences.

"""

#split train/val + test inputs
X_tr_val_inputs, X_test_inputs, y_tr_val_tags, y_test_tags = train_test_split(input_ids, tags,
                                                            random_state=9, test_size=0.2)
#split train + val 
X_tr_inputs, X_val_inputs, y_tr_tags, y_val_tags = train_test_split(X_tr_val_inputs, y_tr_val_tags,
                                                            random_state=9, test_size=0.1)
#split tr/val + test masks
attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]
tr_val_masks, test_masks, _, _ = train_test_split(attention_masks, input_ids,
                                             random_state=9, test_size=0.2)
#split tr + val masks
split_at_mask = [[float(i != 0.0) for i in ii] for ii in tr_val_masks]
tr_masks, val_masks, _, _ = train_test_split(split_at_mask, tr_val_masks,
                                             random_state=9, test_size=0.1)

#split into 3 train/val/test 
#split 80 train, and 20 test. Split 80 into 90 train and 10 val
#after each epoch select best epoch from validation and use the best for testing

tr_inputs = torch.tensor(X_tr_inputs)
val_inputs = torch.tensor(X_val_inputs)
test_inputs = torch.tensor(X_test_inputs)

tr_tags = torch.tensor(y_tr_tags)
val_tags = torch.tensor(y_val_tags)
test_tags = torch.tensor(y_test_tags)

tr_masks = torch.tensor(tr_masks)
val_masks = torch.tensor(val_masks)
test_masks = torch.tensor(test_masks)

# wrap tensors
train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)
valid_data = TensorDataset(val_inputs, val_masks, val_tags)
test_data = TensorDataset(test_inputs, test_masks, test_tags)

# samplers for sampling the data during training, validation and testing
sampler = RandomSampler(train_data)
val_sampler = RandomSampler(valid_data)
test_sampler = SequentialSampler(test_data)

# dataLoader for train, validation and test sets
training_data_loader = DataLoader(train_data, sampler=sampler, batch_size=bs)
validation_data_loader = DataLoader(valid_data, sampler=val_sampler, batch_size=bs)
test_data_loader = DataLoader(test_data, sampler=test_sampler)

"""##Train

BertForTokenClassification is a 
fine-tuning model that wraps BertModel and adds token-level classifier on top of the BertModel. The token-level classifier is a linear layer that takes as input the last hidden state of the sequence
"""

len(tag2idx)

model = AutoModelForTokenClassification.from_pretrained(
    #"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
    #"bert-base-uncased" ,
    #"emilyalsentzer/Bio_ClinicalBERT",
    #"dmis-lab/biobert-base-cased-v1.2",
    #'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12',
    '/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/IHC_BERT/weights/checkpoint-380000',
    num_labels=len(tag2idx),
    output_attentions = False,
    output_hidden_states = False
)
model.cuda();

"""Select GPU if available"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

from transformers import AdamW
FULL_FINETUNING = True
if FULL_FINETUNING:
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'gamma', 'beta']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.0}
    ]
else:
    param_optimizer = list(model.classifier.named_parameters())
    optimizer_grouped_parameters = [{"params": [p for n, p in param_optimizer]}]

optimizer = AdamW(
    optimizer_grouped_parameters,
    lr=3e-5,
    eps=1e-8
)

from transformers import get_linear_schedule_with_warmup

epochs = 10 #100,200
max_grad_norm = 1.0

# Total number of training steps is number of batches * number of epochs.
total_steps = len(training_data_loader) * epochs

# Create the learning rate scheduler.
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

no_o = tag_ls[0:-1]

no_o

"""labels = [  'B-HPI_pos', 'B-HPI_neg', 'I-HPI_neg', 'B-HPI_qualified',
       'B-Descriptor', 'I-Descriptor', 'B-HPI_irrelevant', 'I-HPI_pos',
       'I-HPI_qualified', 'I-HPI_irrelevant', 'B-Molecular_neg',
       'B-Molecular_pos', '', 'I-Molecular_pos', 'I-Molecular_neg',
       'B-Body Part, Organ, or Organ Component_pos',
       'I-Iody Part, Organ, or Organ Component_pos',
       'B-Neoplastic Process', 'I-Neoplastic Process', 'B-Modifier',
       'I-Modifier', 'B-Neoplastic Process_pos',
       'I-Neoplastic Process_pos', 'B-Diagnostic Procedure_pos',
       'I-Diagnostic Procedure_pos']

Weighted Average
"""

## Store the average loss after each epoch so we can plot them.

import copy
loss_values, validation_loss_values, f1_val = [], [],[]
from sklearn.metrics import f1_score, classification_report
best_model_state = None
best_score = 1
for _ in trange(epochs, desc="Epoch"):
    # ========================================
    #               Training
    # ========================================
    # Perform one full pass over the training set.

    # Put the model into training mode.
    model.train()


    # Reset the total loss for this epoch.
    total_loss = 0

    # Training loop
    for step, batch in enumerate(training_data_loader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        # Always clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()
        # forward pass
        # This will return the loss (rather than the model output)
        # because we have provided the `labels`.
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask, labels=b_labels)
        # get the loss
        loss = outputs[0]
        # Perform a backward pass to calculate the gradients.
        loss.backward()
        # track train loss
        total_loss += loss.item()
        # Clip the norm of the gradient
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        # Update the learning rate.
        scheduler.step()
        

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(training_data_loader)
    print("Average train loss: {}".format(avg_train_loss))

    

    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)


    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    # Put the model into evaluation mode
    model.eval()
    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in validation_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)

    eval_loss = eval_loss / len(validation_data_loader) 
    validation_loss_values.append(eval_loss)
    print("Validation loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]
   
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]#[PAD]                             
    valid_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]#[PAD]
    f1_val.append(f1_score(pred_tags, valid_tags, average = 'macro',labels = tag_ls))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    try:
      print("Validation Accuracy: {}".format(accuracy_score(pred_tags, valid_tags)))
      #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
      #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
      print("Validation macro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'macro', labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
      print('Classification Report: {}'.format(classification_report(pred_tags, valid_tags,labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    

     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=tag_ls, zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=['B-HPI_pos','B-HPI_neg'], zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',

      print()

      print()
      mistakes = []
      predicted_tags = []
      true_tags = []
      for p, v in zip(pred_tags, valid_tags):
        predicted_tags.append(p)
        true_tags.append(v)
        if p != v:
          mistakes.append(p)
          mistakes.append(v)
          
      #print(f'predicted tags:{pred_tags}')
      #print(f'true tags:{valid_tags}')
      #print(f"mistakes list:{mistakes}")
      print(f'number of mistakes:{int(len(mistakes)/2)}')
      print('\n')

      if eval_loss < best_score and eval_loss > avg_train_loss:
          best_score = eval_loss
          best_model_state = copy.deepcopy(model.state_dict())
          print('current best evaluation loss is {0:.3f}'.format(best_score))
          print('\n')
    except:
      continue
print(f"final lowest evaluation loss is {best_score}")#excluding O class
    


 #if token is predicted incorectly, print token and surrounding tokens

## Store the average loss after each epoch so we can plot them.
'''
import copy
loss_values, validation_loss_values, f1_val = [], [],[]
from sklearn.metrics import f1_score, classification_report
best_model_state = None
best_score = -1
for _ in trange(epochs, desc="Epoch"):
    # ========================================
    #               Training
    # ========================================
    # Perform one full pass over the training set.

    # Put the model into training mode.
    model.train()


    # Reset the total loss for this epoch.
    total_loss = 0

    # Training loop
    for step, batch in enumerate(training_data_loader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        # Always clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()
        # forward pass
        # This will return the loss (rather than the model output)
        # because we have provided the `labels`.
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask, labels=b_labels)
        # get the loss
        loss = outputs[0]
        # Perform a backward pass to calculate the gradients.
        loss.backward()
        # track train loss
        total_loss += loss.item()
        # Clip the norm of the gradient
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        # Update the learning rate.
        scheduler.step()
        

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(training_data_loader)
    print("Average train loss: {}".format(avg_train_loss))

    

    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)


    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    # Put the model into evaluation mode
    model.eval()
    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in validation_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)

    eval_loss = eval_loss / len(validation_data_loader) 
    validation_loss_values.append(eval_loss)
    print("Validation loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]
   
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]#[PAD]                             
    valid_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]#[PAD]
    f1_val.append(f1_score(pred_tags, valid_tags, average = 'macro',labels = no_o))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    try:
      print("Validation Accuracy: {}".format(accuracy_score(pred_tags, valid_tags)))
      #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
      #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
      print("Validation macro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'macro', labels=no_o, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
      print('Classification Report: {}'.format(classification_report(pred_tags, valid_tags,labels=no_o, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    

      ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=['B-HPI_pos','B-HPI_neg'], zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      print()

      print()
      mistakes = []
      predicted_tags = []
      true_tags = []
      for p, v in zip(pred_tags, valid_tags):
        predicted_tags.append(p)
        true_tags.append(v)
        if p != v:
          mistakes.append(p)
          mistakes.append(v)
          
      #print(f'predicted tags:{pred_tags}')
      #print(f'true tags:{valid_tags}')
      #print(f"mistakes list:{mistakes}")
      print(f'number of mistakes:{int(len(mistakes)/2)}')
      print('\n')

      if best_score < ret:
          best_score = ret
          best_model_state = copy.deepcopy(model.state_dict())
          print('current best score is {0:.3f}'.format(best_score))
          print('\n')
    except:
      continue
print(f"final best F1 macro score is {best_score}")#excluding O class
    


 #if token is predicted incorectly, print token and surrounding tokens   
    '''

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

import seaborn as sns

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(loss_values, 'b-o', label="training loss")
plt.plot(validation_loss_values, 'r-o', label="validation loss")

# Label the plot.
plt.title("Learning curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.savefig('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/ihc_loss_evloss.png')
plt.show()

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(f1_val, 'g-o', label="validation f1")
# Label the plot.
plt.title("Learning curve")
plt.xlabel("Epoch")
plt.ylabel("F1 Weigthed Average Score")
plt.legend()
plt.savefig('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/ihc_f_score_evloss.png')
plt.show()

"""Save the model"""

#add timestamp to file name to track the version of weights saved
t = time.localtime()
timestamp = time.strftime('%d_%b_%H_%M', t)
model_file = ('ihc_best_model_evloss'+timestamp+'.pt')
#path = f"./{model_file}"
path =f"/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/{model_file}"
#torch.save(model.state_dict(), path)
torch.save(best_model_state, path)

print(timestamp)

"""##Evaluate"""

model_file = ('ihc_best_model_evloss'+timestamp+'.pt')
path =f"/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/{model_file}"

best_model = BertForTokenClassification.from_pretrained(
    #"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
    #"emilyalsentzer/Bio_ClinicalBERT",
    #"bert-base-uncased",
    '/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/IHC_BERT/weights/checkpoint-380000',
    #"dmis-lab/biobert-base-cased-v1.2",
    num_labels=len(tag2idx),
    output_attentions = False,
    output_hidden_states = False
)
best_model.to(device)
#best_model.load_state_dict(best_model_state)
best_model.load_state_dict(torch.load(path))

from sklearn.metrics import f1_score, classification_report
    loss_values, test_loss_values = [], []
    best_model.eval()
    # Reset the validation loss for this epoch.

    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels, tokens = [], [],[]
    for batch in test_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)

        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = best_model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        token_list = tokenizer.convert_ids_to_tokens(b_input_ids.to('cpu').numpy()[0])

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)
        tokens.extend(token_list)

    eval_loss = eval_loss / len(test_data_loader) 
    test_loss_values.append(eval_loss)


    print("Test loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]

    print(len(predictions))
    print(len(true_labels))
    print(predictions[0])
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[p_i] != "PAD"] 
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]                            
    test_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]
    print(len(pred_tags))
    print(len(test_tags))                              
    print("Test Accuracy: {}".format(accuracy_score(pred_tags, test_tags)))
    #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
    #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
    print("Test Macro F1-Score: {}".format(f1_score(pred_tags, test_tags, average = 'macro', labels =no_o)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    
    
    #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
    print('Classification Report: {}'.format(classification_report(pred_tags, test_tags, labels = no_o)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    #ret = (f1_score(pred_tags, test_tags, average = 'macro'))
    print()

    print()
    mistakes = []
    predicted_tags = []
    true_tags = []
    token_mistakes =[]
    
    #print(len(tokens))
    
    pred_tags = [tag_values[p_i] for p in predictions
                                  for p_i in p if tag_values[p_i] != "[PAD]"]   #[PAD]                          
    test_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"] #[PAD]
    for idx, (p, v) in enumerate(zip(pred_tags, test_tags)):
      predicted_tags.append(p)
      true_tags.append(v)
      if p != v :#and p != 'PAD':

        mistakes.append(p)
        mistakes.append(v)
        mistakes.append(tokens[idx-1])
        mistakes.append(tokens[idx])
        mistakes.append(tokens[idx+1])
        token_mistakes.append(tokens[idx])
    #print(f'predicted tags:{pred_tags}')
    #print(f'true tags:{valid_tags}')
    print(f"(predicted, true, before_mistake, token, after_mistake):{mistakes}")
    print(f'number of mistakes:{int(len(mistakes)/3)}')

    print(f'List of tokens predicted incorrectly:{token_mistakes}')

"""# Bio Clinical BERT

##Split Train/Test
"""

#tokenizer = BertTokenizerFast.from_pretrained("microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract", do_lower_case=False)
#tokenizer = BertTokenizerFast.from_pretrained("bert-base-cased", do_lower_case=False)

#tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizerFast.from_pretrained("emilyalsentzer/Bio_ClinicalBERT", do_lower_case=False)
#tokenizer = BertTokenizerFast.from_pretrained( "dmis-lab/biobert-base-cased-v1.2", do_lower_case=True)
#tokenizer = BertTokenizerFast.from_pretrained( "dmis-lab/biobert-base-cased-v1.2", do_lower_case=False)

def tokenize_and_preserve_labels(sentence, text_labels):
    tokenized_sentence = []
    labels = []

    for word, label in zip(sentence, text_labels):

        # Tokenize the word and count # of subwords the word is broken into
        tokenized_word = tokenizer.tokenize(word)
        n_subwords = len(tokenized_word)

        # Add the tokenized word to the final tokenized word list
        tokenized_sentence.extend(tokenized_word)

        # Add the same label to the new list of labels `n_subwords` times
        labels.extend([label] * n_subwords)

    return tokenized_sentence, labels

tokenized_texts_and_labels = [
    tokenize_and_preserve_labels(sent, labs)
    for sent, labs in zip(sentences, labels)
]

tokenized_texts_and_labels[0]

tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]
labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]


print(tokenized_texts)
print(labels)

data.Tag.unique()

#padding for sentences with maximum length 200
input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],
                          maxlen=max_len, dtype="long", value=0.0,
                          truncating="post", padding="post")
input_ids[2]

#padding for labels
tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],
                     maxlen=max_len, value=tag2idx["PAD"], padding="post",
                     dtype="long", truncating="post")


#tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],
 #                    maxlen=max_len, value=tag2idx["O"], padding="post",
  #                   dtype="long", truncating="post")

"""Loading data for training and testing.Creating the mask to ignore the padded elements in the sequences.

"""

#split train/val + test inputs
X_tr_val_inputs, X_test_inputs, y_tr_val_tags, y_test_tags = train_test_split(input_ids, tags,
                                                            random_state=9, test_size=0.2)
#split train + val 
X_tr_inputs, X_val_inputs, y_tr_tags, y_val_tags = train_test_split(X_tr_val_inputs, y_tr_val_tags,
                                                            random_state=9, test_size=0.1)
#split tr/val + test masks
attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]
tr_val_masks, test_masks, _, _ = train_test_split(attention_masks, input_ids,
                                             random_state=9, test_size=0.2)
#split tr + val masks
split_at_mask = [[float(i != 0.0) for i in ii] for ii in tr_val_masks]
tr_masks, val_masks, _, _ = train_test_split(split_at_mask, tr_val_masks,
                                             random_state=9, test_size=0.1)

#split into 3 train/val/test 
#split 80 train, and 20 test. Split 80 into 90 train and 10 val
#after each epoch select best epoch from validation and use the best for testing

tr_inputs = torch.tensor(X_tr_inputs)
val_inputs = torch.tensor(X_val_inputs)
test_inputs = torch.tensor(X_test_inputs)

tr_tags = torch.tensor(y_tr_tags)
val_tags = torch.tensor(y_val_tags)
test_tags = torch.tensor(y_test_tags)

tr_masks = torch.tensor(tr_masks)
val_masks = torch.tensor(val_masks)
test_masks = torch.tensor(test_masks)

# wrap tensors
train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)
valid_data = TensorDataset(val_inputs, val_masks, val_tags)
test_data = TensorDataset(test_inputs, test_masks, test_tags)

# samplers for sampling the data during training, validation and testing
sampler = RandomSampler(train_data)
val_sampler = RandomSampler(valid_data)
test_sampler = SequentialSampler(test_data)

# dataLoader for train, validation and test sets
training_data_loader = DataLoader(train_data, sampler=sampler, batch_size=bs)
validation_data_loader = DataLoader(valid_data, sampler=val_sampler, batch_size=bs)
test_data_loader = DataLoader(test_data, sampler=test_sampler)

"""##Train

BertForTokenClassification is a 
fine-tuning model that wraps BertModel and adds token-level classifier on top of the BertModel. The token-level classifier is a linear layer that takes as input the last hidden state of the sequence
"""

import transformers
from transformers import BertForTokenClassification, AdamW

transformers.__version__

len(tag2idx)

model = BertForTokenClassification.from_pretrained(
    #"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
    #"bert-base-uncased" ,
    "emilyalsentzer/Bio_ClinicalBERT",
    #"dmis-lab/biobert-base-cased-v1.2",
    num_labels=len(tag2idx),
    output_attentions = False,
    output_hidden_states = False
)
model.cuda();

"""Select GPU if available"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

from transformers import AdamW
FULL_FINETUNING = True
if FULL_FINETUNING:
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'gamma', 'beta']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.0}
    ]
else:
    param_optimizer = list(model.classifier.named_parameters())
    optimizer_grouped_parameters = [{"params": [p for n, p in param_optimizer]}]

optimizer = AdamW(
    optimizer_grouped_parameters,
    lr=3e-5,
    eps=1e-8
)

from transformers import get_linear_schedule_with_warmup

epochs = 10 #100,200
max_grad_norm = 1.0

# Total number of training steps is number of batches * number of epochs.
total_steps = len(training_data_loader) * epochs

# Create the learning rate scheduler.
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

no_o = tag_ls[0:-1]

no_o

"""labels = [  'B-HPI_pos', 'B-HPI_neg', 'I-HPI_neg', 'B-HPI_qualified',
       'B-Descriptor', 'I-Descriptor', 'B-HPI_irrelevant', 'I-HPI_pos',
       'I-HPI_qualified', 'I-HPI_irrelevant', 'B-Molecular_neg',
       'B-Molecular_pos', '', 'I-Molecular_pos', 'I-Molecular_neg',
       'B-Body Part, Organ, or Organ Component_pos',
       'I-Iody Part, Organ, or Organ Component_pos',
       'B-Neoplastic Process', 'I-Neoplastic Process', 'B-Modifier',
       'I-Modifier', 'B-Neoplastic Process_pos',
       'I-Neoplastic Process_pos', 'B-Diagnostic Procedure_pos',
       'I-Diagnostic Procedure_pos']

Weighted Average
"""

## Store the average loss after each epoch so we can plot them.

import copy
loss_values, validation_loss_values, f1_val = [], [],[]
from sklearn.metrics import f1_score, classification_report
best_model_state = None
best_score = 1
for _ in trange(epochs, desc="Epoch"):
    # ========================================
    #               Training
    # ========================================
    # Perform one full pass over the training set.

    # Put the model into training mode.
    model.train()


    # Reset the total loss for this epoch.
    total_loss = 0

    # Training loop
    for step, batch in enumerate(training_data_loader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        # Always clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()
        # forward pass
        # This will return the loss (rather than the model output)
        # because we have provided the `labels`.
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask, labels=b_labels)
        # get the loss
        loss = outputs[0]
        # Perform a backward pass to calculate the gradients.
        loss.backward()
        # track train loss
        total_loss += loss.item()
        # Clip the norm of the gradient
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        # Update the learning rate.
        scheduler.step()
        

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(training_data_loader)
    print("Average train loss: {}".format(avg_train_loss))

    

    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)


    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    # Put the model into evaluation mode
    model.eval()
    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in validation_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)

    eval_loss = eval_loss / len(validation_data_loader) 
    validation_loss_values.append(eval_loss)
    print("Validation loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]
   
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]#[PAD]                             
    valid_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]#[PAD]
    f1_val.append(f1_score(pred_tags, valid_tags, average = 'macro',labels = tag_ls))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    try:
      print("Validation Accuracy: {}".format(accuracy_score(pred_tags, valid_tags)))
      #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
      #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
      print("Validation macro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'macro', labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
      print('Classification Report: {}'.format(classification_report(pred_tags, valid_tags,labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    

     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=tag_ls, zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=['B-HPI_pos','B-HPI_neg'], zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',

      print()

      print()
      mistakes = []
      predicted_tags = []
      true_tags = []
      for p, v in zip(pred_tags, valid_tags):
        predicted_tags.append(p)
        true_tags.append(v)
        if p != v:
          mistakes.append(p)
          mistakes.append(v)
          
      #print(f'predicted tags:{pred_tags}')
      #print(f'true tags:{valid_tags}')
      #print(f"mistakes list:{mistakes}")
      print(f'number of mistakes:{int(len(mistakes)/2)}')
      print('\n')

      if eval_loss < best_score and eval_loss > avg_train_loss:
          best_score = eval_loss
          best_model_state = copy.deepcopy(model.state_dict())
          print('current best evaluation loss is {0:.3f}'.format(best_score))
          print('\n')
    except:
      continue
print(f"final lowest evaluation loss is {best_score}")#excluding O class
    


 #if token is predicted incorectly, print token and surrounding tokens

## Store the average loss after each epoch so we can plot them.
'''
import copy
loss_values, validation_loss_values, f1_val = [], [],[]
from sklearn.metrics import f1_score, classification_report
best_model_state = None
best_score = -1
for _ in trange(epochs, desc="Epoch"):
    # ========================================
    #               Training
    # ========================================
    # Perform one full pass over the training set.

    # Put the model into training mode.
    model.train()


    # Reset the total loss for this epoch.
    total_loss = 0

    # Training loop
    for step, batch in enumerate(training_data_loader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        # Always clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()
        # forward pass
        # This will return the loss (rather than the model output)
        # because we have provided the `labels`.
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask, labels=b_labels)
        # get the loss
        loss = outputs[0]
        # Perform a backward pass to calculate the gradients.
        loss.backward()
        # track train loss
        total_loss += loss.item()
        # Clip the norm of the gradient
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        # Update the learning rate.
        scheduler.step()
        

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(training_data_loader)
    print("Average train loss: {}".format(avg_train_loss))

    

    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)


    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    # Put the model into evaluation mode
    model.eval()
    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in validation_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)

    eval_loss = eval_loss / len(validation_data_loader) 
    validation_loss_values.append(eval_loss)
    print("Validation loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]
   
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]#[PAD]                             
    valid_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]#[PAD]
    f1_val.append(f1_score(pred_tags, valid_tags, average = 'macro',labels = no_o))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    try:
      print("Validation Accuracy: {}".format(accuracy_score(pred_tags, valid_tags)))
      #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
      #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
      print("Validation macro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'macro', labels=no_o, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
      print('Classification Report: {}'.format(classification_report(pred_tags, valid_tags,labels=no_o, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    

      ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=no_o, zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      print()

      print()
      mistakes = []
      predicted_tags = []
      true_tags = []
      for p, v in zip(pred_tags, valid_tags):
        predicted_tags.append(p)
        true_tags.append(v)
        if p != v:
          mistakes.append(p)
          mistakes.append(v)
          
      #print(f'predicted tags:{pred_tags}')
      #print(f'true tags:{valid_tags}')
      #print(f"mistakes list:{mistakes}")
      print(f'number of mistakes:{int(len(mistakes)/2)}')
      print('\n')

      if best_score < ret:
          best_score = ret
          best_model_state = copy.deepcopy(model.state_dict())
          print('current best score is {0:.3f}'.format(best_score))
          print('\n')
    except:
      continue
print(f"final best F1 macro score is {best_score}")#excluding O class
    


 #if token is predicted incorectly, print token and surrounding tokens   
    
'''

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

import seaborn as sns

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(loss_values, 'b-o', label="training loss")
plt.plot(validation_loss_values, 'r-o', label="validation loss")

# Label the plot.
plt.title("Learning curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.savefig('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/bio_loss.png')
plt.show()

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(f1_val, 'g-o', label="validation f1")
# Label the plot.
plt.title("Learning curve")
plt.xlabel("Epoch")
plt.ylabel("F1 Weigthed Average Score")
plt.legend()
plt.savefig('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/bio_f_score.png')
plt.show()

"""Save the model"""

#add timestamp to file name to track the version of weights saved
t = time.localtime()
timestamp = time.strftime('%d_%b_%H_%M', t)
model_file = ('bio_best_model'+timestamp+'.pt')
#path = f"./{model_file}"
path =f"/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/{model_file}"
#torch.save(model.state_dict(), path)
torch.save(best_model_state, path)

print(timestamp)

"""##Evaluate"""

from google.colab import drive
drive.mount('/content/drive')

timestamp='15_Sep_19_35'

model_file = ('bio_best_model'+timestamp+'.pt')

path =f"/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/{model_file}"

best_model = BertForTokenClassification.from_pretrained(
    #"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
    "emilyalsentzer/Bio_ClinicalBERT",
    #"bert-base-uncased",
    
    #"dmis-lab/biobert-base-cased-v1.2",
    num_labels=len(tag2idx),
    output_attentions = False,
    output_hidden_states = False
)
best_model.to(device)
#best_model.load_state_dict(best_model_state)
best_model.load_state_dict(torch.load(path))

from sklearn.metrics import f1_score, classification_report
    loss_values, test_loss_values = [], []
    best_model.eval()
    # Reset the validation loss for this epoch.

    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels, tokens = [], [],[]
    for batch in test_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)

        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = best_model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        token_list = tokenizer.convert_ids_to_tokens(b_input_ids.to('cpu').numpy()[0])

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)
        tokens.extend(token_list)

    eval_loss = eval_loss / len(test_data_loader) 
    test_loss_values.append(eval_loss)


    print("Test loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]

    print(len(predictions))
    print(len(true_labels))
    print(predictions[0])
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[p_i] != "PAD"] 
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]                            
    test_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]
    print(len(pred_tags))
    print(len(test_tags))                              
    print("Test Accuracy: {}".format(accuracy_score(pred_tags, test_tags)))
    #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
    #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
    print("Test weighted F1-Score: {}".format(f1_score(pred_tags, test_tags, average = 'weighted', labels =no_o)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    
    
    #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
    print('Classification Report: {}'.format(classification_report(pred_tags, test_tags, labels = no_o)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    #ret = (f1_score(pred_tags, test_tags, average = 'macro'))
    print()

    print()
    mistakes = []
    predicted_tags = []
    true_tags = []
    token_mistakes =[]
    
    #print(len(tokens))
    
    pred_tags = [tag_values[p_i] for p in predictions
                                  for p_i in p if tag_values[p_i] != "[PAD]"]   #[PAD]                          
    test_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"] #[PAD]
    for idx, (p, v) in enumerate(zip(pred_tags, test_tags)):
      predicted_tags.append(p)
      true_tags.append(v)
      if p != v :#and p != 'PAD':

        mistakes.append(p)
        mistakes.append(v)
        mistakes.append(tokens[idx-1])
        mistakes.append(tokens[idx])
        mistakes.append(tokens[idx+1])
        token_mistakes.append(tokens[idx])
    #print(f'predicted tags:{pred_tags}')
    #print(f'true tags:{valid_tags}')
    print(f"(predicted, true, before_mistake, token, after_mistake):{mistakes}")
    print(f'number of mistakes:{int(len(mistakes)/3)}')

    print(f'List of tokens predicted incorrectly:{token_mistakes}')

### Confusion Matrix
from sklearn.metrics import confusion_matrix
predictions = model.predict(x_test, steps=len(x_test), verbose=0)
#y_pred=model.predict(x_test)
#y_pred = np.round(y_pred)
y_pred = np.argmax(predictions, axis=-1)

y_true=np.argmax(y_test, axis=-1)

cm = confusion_matrix(y_true, y_pred)

## Get Class Labels
labels = le.classes_
class_names = labels

# Plot confusion matrix in a beautiful manner
fig = plt.figure(figsize=(16, 14))
ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells
# labels, title and ticks
ax.set_xlabel('Predicted', fontsize=20)
ax.xaxis.set_label_position('bottom')
plt.xticks(rotation=90)
ax.xaxis.set_ticklabels(class_names, fontsize = 10)
ax.xaxis.tick_bottom()

ax.set_ylabel('True', fontsize=20)
ax.yaxis.set_ticklabels(class_names, fontsize = 10)
plt.yticks(rotation=0)

plt.title('Refined Confusion Matrix', fontsize=20)

plt.savefig('ConMat24.png')
plt.show()

y_bi = MultiLabelBinarizer().fit_transform(test_tags)
pred_bi = MultiLabelBinarizer().fit_transform(pred_tags)
ax = plt.axes()
cm = confusion_matrix(y_bi.argmax(axis=-1), pred_bi.argmax(axis=-1))
sns.heatmap(cm, annot=True, fmt='d')
plt.title('Bio_ClinicalBERT Confusion Matrix')
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
ax.xaxis.set_ticklabels(no_o)
ax.yaxis.set_ticklabels(no_o)
plt.show()

tag_ls

"""Evaluate

# IHC-Bio/Clinical BERT

##Split Train/Test
"""

#tokenizer = BertTokenizerFast.from_pretrained("microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract", do_lower_case=False)
#tokenizer = BertTokenizerFast.from_pretrained("bert-base-cased", do_lower_case=False)

tokenizer = BertTokenizerFast.from_pretrained("/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Clinical_IHC_BERT/weights/checkpoint-400000",do_lower_case=True)
#tokenizer = BertTokenizerFast.from_pretrained("emilyalsentzer/Bio_ClinicalBERT", do_lower_case=False)
#tokenizer = BertTokenizerFast.from_pretrained( "dmis-lab/biobert-base-cased-v1.2", do_lower_case=True)
#tokenizer = BertTokenizerFast.from_pretrained( "dmis-lab/biobert-base-cased-v1.2", do_lower_case=False)
#tokenizer = BertTokenizerFast.from_pretrained("cambridgeltl/SapBERT-from-PubMedBERT-fulltext")

def tokenize_and_preserve_labels(sentence, text_labels):
    tokenized_sentence = []
    labels = []

    for word, label in zip(sentence, text_labels):

        # Tokenize the word and count # of subwords the word is broken into
        tokenized_word = tokenizer.tokenize(word)
        n_subwords = len(tokenized_word)

        # Add the tokenized word to the final tokenized word list
        tokenized_sentence.extend(tokenized_word)

        # Add the same label to the new list of labels `n_subwords` times
        labels.extend([label] * n_subwords)

    return tokenized_sentence, labels

tokenized_texts_and_labels = [
    tokenize_and_preserve_labels(sent, labs)
    for sent, labs in zip(sentences, labels)
]

tokenized_texts_and_labels[11]

tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]
labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]

data.Tag.unique()

#padding for sentences with maximum length 200
input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],
                          maxlen=max_len, dtype="long", value=0.0,
                          truncating="post", padding="post")
input_ids[2]

#padding for labels
tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],
                     maxlen=max_len, value=tag2idx["PAD"], padding="post",
                     dtype="long", truncating="post")


#tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],
 #                    maxlen=max_len, value=tag2idx["O"], padding="post",
  #                   dtype="long", truncating="post")

len(input_ids)

"""Loading data for training and testing.Creating the mask to ignore the padded elements in the sequences.

"""

#split train/val + test inputs
indices = np.arange(len(input_ids))
X_tr_val_inputs, X_test_inputs, y_tr_val_tags, y_test_tags, indices_train,indices_test = train_test_split(input_ids, tags,indices,random_state=9, test_size=0.2)
#split train + val 
X_tr_inputs, X_val_inputs, y_tr_tags, y_val_tags = train_test_split(X_tr_val_inputs, y_tr_val_tags,
                                                            random_state=9, test_size=0.1)
#split tr/val + test masks
attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]
tr_val_masks, test_masks, _, _ = train_test_split(attention_masks, input_ids,
                                             random_state=9, test_size=0.2)
#split tr + val masks
split_at_mask = [[float(i != 0.0) for i in ii] for ii in tr_val_masks]
tr_masks, val_masks, _, _ = train_test_split(split_at_mask, tr_val_masks,
                                             random_state=9, test_size=0.1)

#split into 3 train/val/test 
#split 80 train, and 20 test. Split 80 into 90 train and 10 val
#after each epoch select best epoch from validation and use the best for testing

len(X_tr_val_inputs)

len(X_test_inputs)

tr_inputs = torch.tensor(X_tr_inputs)
val_inputs = torch.tensor(X_val_inputs)
test_inputs = torch.tensor(X_test_inputs)

tr_tags = torch.tensor(y_tr_tags)
val_tags = torch.tensor(y_val_tags)
test_tags = torch.tensor(y_test_tags)

tr_masks = torch.tensor(tr_masks)
val_masks = torch.tensor(val_masks)
test_masks = torch.tensor(test_masks)

# wrap tensors
train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)
valid_data = TensorDataset(val_inputs, val_masks, val_tags)
test_data = TensorDataset(test_inputs, test_masks, test_tags)

# samplers for sampling the data during training, validation and testing
sampler = RandomSampler(train_data)
val_sampler = RandomSampler(valid_data)
test_sampler = SequentialSampler(test_data)

# dataLoader for train, validation and test sets
training_data_loader = DataLoader(train_data, sampler=sampler, batch_size=bs)
validation_data_loader = DataLoader(valid_data, sampler=val_sampler, batch_size=bs)
test_data_loader = DataLoader(test_data, sampler=test_sampler)

"""##Train

BertForTokenClassification is a 
fine-tuning model that wraps BertModel and adds token-level classifier on top of the BertModel. The token-level classifier is a linear layer that takes as input the last hidden state of the sequence
"""

len(tag2idx)

tag2idx

model = BertForTokenClassification.from_pretrained(
    #"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
    #"bert-base-uncased" ,
    #"emilyalsentzer/Bio_ClinicalBERT",
    #"dmis-lab/biobert-base-cased-v1.2",
    #'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12',
    '/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Clinical_IHC_BERT/weights/checkpoint-400000',
    num_labels=len(tag2idx),
    output_attentions = True,
    output_hidden_states = True
)
model.cuda();

"""Select GPU if available"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

from transformers import AdamW
FULL_FINETUNING = True
if FULL_FINETUNING:
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'gamma', 'beta']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.0}
    ]
else:
    param_optimizer = list(model.classifier.named_parameters())
    optimizer_grouped_parameters = [{"params": [p for n, p in param_optimizer]}]

optimizer = AdamW(
    optimizer_grouped_parameters,
    lr=3e-5,
    eps=1e-8
)

from transformers import get_linear_schedule_with_warmup

epochs = 10 #100,200
max_grad_norm = 1.0

# Total number of training steps is number of batches * number of epochs.
total_steps = len(training_data_loader) * epochs

# Create the learning rate scheduler.
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

no_o = tag_ls[0:-1]

tag_ls

""" labels = [  'B-HPI_pos', 'B-HPI_neg', 'I-HPI_neg', 'B-HPI_qualified',
       'B-Descriptor', 'I-Descriptor', 'B-HPI_irrelevant', 'I-HPI_pos',
       'I-HPI_qualified', 'I-HPI_irrelevant', 'B-Molecular_neg',
       'B-Molecular_pos', '', 'I-Molecular_pos', 'I-Molecular_neg',
       'B-Body Part, Organ, or Organ Component_pos',
       'I-Iody Part, Organ, or Organ Component_pos',
       'B-Neoplastic Process', 'I-Neoplastic Process', 'B-Modifier',
       'I-Modifier', 'B-Neoplastic Process_pos',
       'I-Neoplastic Process_pos', 'B-Diagnostic Procedure_pos',
       'I-Diagnostic Procedure_pos']
"""

## Store the average loss after each epoch so we can plot them.

import copy
loss_values, validation_loss_values, f1_val = [], [],[]
from sklearn.metrics import f1_score, classification_report
best_model_state = None
best_score = 1
for _ in trange(epochs, desc="Epoch"):
    # ========================================
    #               Training
    # ========================================
    # Perform one full pass over the training set.

    # Put the model into training mode.
    model.train()


    # Reset the total loss for this epoch.
    total_loss = 0

    # Training loop
    for step, batch in enumerate(training_data_loader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        # Always clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()
        # forward pass
        # This will return the loss (rather than the model output)
        # because we have provided the `labels`.
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask, labels=b_labels)
        # get the loss
        loss = outputs[0]
        # Perform a backward pass to calculate the gradients.
        loss.backward()
        # track train loss
        total_loss += loss.item()
        # Clip the norm of the gradient
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        # Update the learning rate.
        scheduler.step()
        

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(training_data_loader)
    print("Average train loss: {}".format(avg_train_loss))

    

    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)


    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    # Put the model into evaluation mode
    model.eval()
    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in validation_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)

    eval_loss = eval_loss / len(validation_data_loader) 
    #if eval_loss< best_score :
      #best_score = eval_loss
    validation_loss_values.append(eval_loss)
    print("Validation loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]
   
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]#[PAD]                             
    valid_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]#[PAD]
    f1_val.append(f1_score(pred_tags, valid_tags, average = 'macro',labels = tag_ls))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    try:
      print("Validation Accuracy: {}".format(accuracy_score(pred_tags, valid_tags)))
      #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
      #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
      print("Validation macro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'macro', labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
      print('Classification Report: {}'.format(classification_report(pred_tags, valid_tags,labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    

     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=tag_ls, zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=['B-HPI_pos','B-HPI_neg'], zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',

      print()

      print()
      mistakes = []
      predicted_tags = []
      true_tags = []
      for p, v in zip(pred_tags, valid_tags):
        predicted_tags.append(p)
        true_tags.append(v)
        if p != v:
          mistakes.append(p)
          mistakes.append(v)
          
      #print(f'predicted tags:{pred_tags}')
      #print(f'true tags:{valid_tags}')
      #print(f"mistakes list:{mistakes}")
      print(f'number of mistakes:{int(len(mistakes)/2)}')
      print('\n')

      if eval_loss < best_score and eval_loss > avg_train_loss:
          best_score = eval_loss
          best_model_state = copy.deepcopy(model.state_dict())
          best_model = copy.deepcopy(model)

          checkpoint = {'model': BertForTokenClassification.from_pretrained(
              '/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Clinical_IHC_BERT/weights/checkpoint-400000',
              num_labels=len(tag2idx),
              output_attentions = True,
              output_hidden_states = True),
              'state_dict': model.state_dict(),
              'optimizer' : optimizer.state_dict()}


          

          
          print('current best evaluation loss is {0:.3f}'.format(best_score))
          print('\n')
    except:
      continue
print(f"final lowest evaluation loss is {best_score}")#excluding O class
    


 #if token is predicted incorectly, print token and surrounding tokens

torch.save(checkpoint, '/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Clinical_IHC_BERT/checkpoint.pth')
print("The state dict keys: \n\n", model.state_dict().keys())

## Store the average loss after each epoch so we can plot them.
'''
import copy
loss_values, validation_loss_values, f1_val = [], [],[]
from sklearn.metrics import f1_score, classification_report
best_model_state = None
best_score = 1
for _ in trange(epochs, desc="Epoch"):
    # ========================================
    #               Training
    # ========================================
    # Perform one full pass over the training set.

    # Put the model into training mode.
    model.train()


    # Reset the total loss for this epoch.
    total_loss = 0

    # Training loop
    for step, batch in enumerate(training_data_loader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        # Always clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()
        # forward pass
        # This will return the loss (rather than the model output)
        # because we have provided the `labels`.
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask, labels=b_labels)
        # get the loss
        loss = outputs[0]
        # Perform a backward pass to calculate the gradients.
        loss.backward()
        # track train loss
        total_loss += loss.item()
        # Clip the norm of the gradient
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        # Update the learning rate.
        scheduler.step()
        

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(training_data_loader)
    print("Average train loss: {}".format(avg_train_loss))

    

    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)


    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    # Put the model into evaluation mode
    model.eval()
    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in validation_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)

    eval_loss = eval_loss / len(validation_data_loader) 
    validation_loss_values.append(eval_loss)
    print("Validation loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]
   
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]#[PAD]                             
    valid_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]#[PAD]
    f1_val.append(f1_score(pred_tags, valid_tags, average = 'macro',labels = tag_ls))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    try:
      print("Validation Accuracy: {}".format(accuracy_score(pred_tags, valid_tags)))
      #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
      #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
      print("Validation macro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'macro', labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
      print('Classification Report: {}'.format(classification_report(pred_tags, valid_tags,labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    

     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=tag_ls, zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=['B-HPI_pos','B-HPI_neg'], zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',

      print()

      print()
      mistakes = []
      predicted_tags = []
      true_tags = []
      for p, v in zip(pred_tags, valid_tags):
        predicted_tags.append(p)
        true_tags.append(v)
        if p != v:
          mistakes.append(p)
          mistakes.append(v)
          
      #print(f'predicted tags:{pred_tags}')
      #print(f'true tags:{valid_tags}')
      #print(f"mistakes list:{mistakes}")
      print(f'number of mistakes:{int(len(mistakes)/2)}')
      print('\n')

      if eval_loss < best_score and eval_loss > avg_train_loss:
          best_score = eval_loss
          best_model_state = copy.deepcopy(model.state_dict())
          best_model = copy.deepcopy(model)


          
          print('current best evaluation loss is {0:.3f}'.format(best_score))
          print('\n')
    except:
      continue
print(f"final lowest evaluation loss is {best_score}")#excluding O class
    


 #if token is predicted incorectly, print token and surrounding tokens   
    '''

## Store the average loss after each epoch so we can plot them.
'''
import copy
loss_values, validation_loss_values, f1_val = [], [],[]
from sklearn.metrics import f1_score, classification_report
best_model_state = None
best_score = -1
for _ in trange(epochs, desc="Epoch"):
    # ========================================
    #               Training
    # ========================================
    # Perform one full pass over the training set.

    # Put the model into training mode.
    model.train()


    # Reset the total loss for this epoch.
    total_loss = 0

    # Training loop
    for step, batch in enumerate(training_data_loader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        # Always clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()
        # forward pass
        # This will return the loss (rather than the model output)
        # because we have provided the `labels`.
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask, labels=b_labels)
        # get the loss
        loss = outputs[0]
        # Perform a backward pass to calculate the gradients.
        loss.backward()
        # track train loss
        total_loss += loss.item()
        # Clip the norm of the gradient
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        # Update the learning rate.
        scheduler.step()
        

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(training_data_loader)
    print("Average train loss: {}".format(avg_train_loss))

    

    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)


    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    # Put the model into evaluation mode
    model.eval()
    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in validation_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)

    eval_loss = eval_loss / len(validation_data_loader) 
    validation_loss_values.append(eval_loss)
    print("Validation loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]
   
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]#[PAD]                             
    valid_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]#[PAD]
    f1_val.append(f1_score(pred_tags, valid_tags, average = 'macro',labels = tag_ls))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    try:
      print("Validation Accuracy: {}".format(accuracy_score(pred_tags, valid_tags)))
      #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
      #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
      print("Validation macro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'macro', labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
      print('Classification Report: {}'.format(classification_report(pred_tags, valid_tags,labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    

     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=tag_ls, zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=['B-HPI_pos','B-HPI_neg'], zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',

      print()

      print()
      mistakes = []
      predicted_tags = []
      true_tags = []
      for p, v in zip(pred_tags, valid_tags):
        predicted_tags.append(p)
        true_tags.append(v)
        if p != v:
          mistakes.append(p)
          mistakes.append(v)
          
      #print(f'predicted tags:{pred_tags}')
      #print(f'true tags:{valid_tags}')
      #print(f"mistakes list:{mistakes}")
      print(f'number of mistakes:{int(len(mistakes)/2)}')
      print('\n')

      if best_score < ret:
          best_score = ret
          best_model_state = copy.deepcopy(model.state_dict())
          print('current best score is {0:.3f}'.format(best_score))
          print('\n')
    except:
      continue
print(f"final best F1 macro score is {best_score}")#excluding O class
    


 #if token is predicted incorectly, print token and surrounding tokens   
    '''

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

import seaborn as sns

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(loss_values, 'b-o', label="training loss")
plt.plot(validation_loss_values, 'r-o', label="validation loss")

# Label the plot.
plt.title("Learning curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.savefig('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/ihc_bio_loss_evloss.png')
plt.show()

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(f1_val, 'g-o', label="validation f1")
# Label the plot.
plt.title("Learning curve")
plt.xlabel("Epoch")
plt.ylabel("F1 Weigthed Average Score")
plt.legend()
plt.savefig('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/ihc_bio_f_score_evloss.png')
plt.show()

"""Save the model"""

#add timestamp to file name to track the version of weights saved
t = time.localtime()
timestamp = time.strftime('%d_%b_%H_%M', t)
model_file = ('ihc_bio_best_model_evloss'+timestamp+'.pt')
#path = f"./{model_file}"
path =f"/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/{model_file}"
#torch.save(model.state_dict(), path)
torch.save(best_model_state, path)

print(timestamp)

"""##Evaluate"""

timestamp

def load_checkpoint(filepath):
    checkpoint = torch.load(filepath)
    model = checkpoint['model']
    model.to(device)
    model.load_state_dict(checkpoint['state_dict'])
    for parameter in model.parameters():
        parameter.requires_grad = False
    
    model.eval()
    
    return model
best_model = load_checkpoint('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Clinical_IHC_BERT/checkpoint.pth')

'''
model_file = ('ihc_bio_best_model_evloss'+timestamp+'.pt')
path =f"/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/{model_file}"

best_model = BertForTokenClassification.from_pretrained(
    #"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
    #"emilyalsentzer/Bio_ClinicalBERT",
    #"bert-base-uncased",
    '/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Clinical_IHC_BERT/weights/checkpoint-380000',
    #"dmis-lab/biobert-base-cased-v1.2",
    num_labels=len(tag2idx),
    output_attentions = False,
    output_hidden_states = False
)
best_model.to(device)
#best_model.load_state_dict(best_model_state)
best_model.load_state_dict(torch.load(path))
best_model.eval()'''

from sklearn.metrics import f1_score, classification_report
    loss_values, test_loss_values = [], []
    best_model.eval()
    # Reset the validation loss for this epoch.

    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels, tokens = [], [],[]
    for batch in test_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)

        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = best_model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        token_list = tokenizer.convert_ids_to_tokens(b_input_ids.to('cpu').numpy()[0])

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)
        tokens.extend(token_list)

    eval_loss = eval_loss / len(test_data_loader) 
    test_loss_values.append(eval_loss)


    print("Test loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]

    print(len(predictions))
    print(len(true_labels))
    print(predictions[0])
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[p_i] != "PAD"] 
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]                            
    test_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]
    print(len(pred_tags))
    print(len(test_tags))                              
    print("Test Accuracy: {}".format(accuracy_score(pred_tags, test_tags)))
    #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
    #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
    print("Test macro F1-Score: {}".format(f1_score(pred_tags, test_tags, average = 'macro', labels =no_o)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    
    
    #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
    print('Classification Report: {}'.format(classification_report(pred_tags, test_tags, labels = no_o)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    #ret = (f1_score(pred_tags, test_tags, average = 'macro'))
    print()

    print()
    mistakes = []
    predicted_tags = []
    true_tags = []
    token_mistakes =[]
    
    #print(len(tokens))
    
    pred_tags = [tag_values[p_i] for p in predictions
                                  for p_i in p if tag_values[p_i] != "[PAD]"]   #[PAD]                          
    test_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"] #[PAD]
    for idx, (p, v) in enumerate(zip(pred_tags, test_tags)):
      predicted_tags.append(p)
      true_tags.append(v)
      if p != v :#and p != 'PAD':

        mistakes.append(p)
        mistakes.append(v)
        mistakes.append(tokens[idx-1])
        mistakes.append(tokens[idx])
        mistakes.append(tokens[idx+1])
        token_mistakes.append(tokens[idx])
    #print(f'predicted tags:{pred_tags}')
    #print(f'true tags:{valid_tags}')
    print(f"(predicted, true, before_mistake, token, after_mistake):{mistakes}")
    print(f'number of mistakes:{int(len(mistakes)/3)}')

    print(f'List of tokens predicted incorrectly:{token_mistakes}')

#y_bi = MultiLabelBinarizer().fit_transform(test_tags)
#pred_bi = MultiLabelBinarizer().fit_transform(pred_tags)
#ax = plt.axes()
#cm = confusion_matrix(y_bi.argmax(axis=-1), pred_bi.argmax(axis=-1))
#sns.heatmap(cm, annot=True, fmt='d')
#plt.title('IHC_Bio_ClinicalBERT Confusion Matrix')
#plt.xlabel('Predicted Class')
#plt.ylabel('True Class')
#ax.xaxis.set_ticklabels(tag_ls)
#ax.yaxis.set_ticklabels(tag_ls)
#plt.show()

"""# Blue BERT

##Split Train/Test
"""

#tokenizer = BertTokenizerFast.from_pretrained("microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract", do_lower_case=False)
#tokenizer = BertTokenizerFast.from_pretrained("bert-base-cased", do_lower_case=False)

tokenizer = BertTokenizerFast.from_pretrained("bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12",do_lower_case=False)
#tokenizer = BertTokenizerFast.from_pretrained("emilyalsentzer/Bio_ClinicalBERT", do_lower_case=False)
#tokenizer = BertTokenizerFast.from_pretrained( "dmis-lab/biobert-base-cased-v1.2", do_lower_case=True)
#tokenizer = BertTokenizerFast.from_pretrained( "dmis-lab/biobert-base-cased-v1.2", do_lower_case=False)

def tokenize_and_preserve_labels(sentence, text_labels):
    tokenized_sentence = []
    labels = []

    for word, label in zip(sentence, text_labels):

        # Tokenize the word and count # of subwords the word is broken into
        tokenized_word = tokenizer.tokenize(word)
        n_subwords = len(tokenized_word)

        # Add the tokenized word to the final tokenized word list
        tokenized_sentence.extend(tokenized_word)

        # Add the same label to the new list of labels `n_subwords` times
        labels.extend([label] * n_subwords)

    return tokenized_sentence, labels

tokenized_texts_and_labels = [
    tokenize_and_preserve_labels(sent, labs)
    for sent, labs in zip(sentences, labels)
]

tokenized_texts_and_labels[0]

tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]
labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]


print(tokenized_texts)
print(labels)

data.Tag.unique()

#padding for sentences with maximum length 200
input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],
                          maxlen=max_len, dtype="long", value=0.0,
                          truncating="post", padding="post")
input_ids[2]

#padding for labels
tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],
                     maxlen=max_len, value=tag2idx["PAD"], padding="post",
                     dtype="long", truncating="post")


#tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],
 #                    maxlen=max_len, value=tag2idx["O"], padding="post",
  #                   dtype="long", truncating="post")

"""Loading data for training and testing.Creating the mask to ignore the padded elements in the sequences.

"""

#split train/val + test inputs
X_tr_val_inputs, X_test_inputs, y_tr_val_tags, y_test_tags = train_test_split(input_ids, tags,
                                                            random_state=9, test_size=0.2)
#split train + val 
X_tr_inputs, X_val_inputs, y_tr_tags, y_val_tags = train_test_split(X_tr_val_inputs, y_tr_val_tags,
                                                            random_state=9, test_size=0.1)
#split tr/val + test masks
attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]
tr_val_masks, test_masks, _, _ = train_test_split(attention_masks, input_ids,
                                             random_state=9, test_size=0.2)
#split tr + val masks
split_at_mask = [[float(i != 0.0) for i in ii] for ii in tr_val_masks]
tr_masks, val_masks, _, _ = train_test_split(split_at_mask, tr_val_masks,
                                             random_state=9, test_size=0.1)

#split into 3 train/val/test 
#split 80 train, and 20 test. Split 80 into 90 train and 10 val
#after each epoch select best epoch from validation and use the best for testing

tr_inputs = torch.tensor(X_tr_inputs)
val_inputs = torch.tensor(X_val_inputs)
test_inputs = torch.tensor(X_test_inputs)

tr_tags = torch.tensor(y_tr_tags)
val_tags = torch.tensor(y_val_tags)
test_tags = torch.tensor(y_test_tags)

tr_masks = torch.tensor(tr_masks)
val_masks = torch.tensor(val_masks)
test_masks = torch.tensor(test_masks)

# wrap tensors
train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)
valid_data = TensorDataset(val_inputs, val_masks, val_tags)
test_data = TensorDataset(test_inputs, test_masks, test_tags)

# samplers for sampling the data during training, validation and testing
sampler = RandomSampler(train_data)
val_sampler = RandomSampler(valid_data)
test_sampler = SequentialSampler(test_data)

# dataLoader for train, validation and test sets
training_data_loader = DataLoader(train_data, sampler=sampler, batch_size=bs)
validation_data_loader = DataLoader(valid_data, sampler=val_sampler, batch_size=bs)
test_data_loader = DataLoader(test_data, sampler=test_sampler)

"""##Train

BertForTokenClassification is a 
fine-tuning model that wraps BertModel and adds token-level classifier on top of the BertModel. The token-level classifier is a linear layer that takes as input the last hidden state of the sequence
"""

import transformers
from transformers import BertForTokenClassification, AdamW

transformers.__version__

len(tag2idx)

model = BertForTokenClassification.from_pretrained(
    #"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
    #"bert-base-uncased" ,
    #"emilyalsentzer/Bio_ClinicalBERT",
    #"dmis-lab/biobert-base-cased-v1.2",
    'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12',
    num_labels=len(tag2idx),
    output_attentions = True,
    output_hidden_states = True
)
model.cuda();

"""Select GPU if available"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

from transformers import AdamW
FULL_FINETUNING = True
if FULL_FINETUNING:
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'gamma', 'beta']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.0}
    ]
else:
    param_optimizer = list(model.classifier.named_parameters())
    optimizer_grouped_parameters = [{"params": [p for n, p in param_optimizer]}]

optimizer = AdamW(
    optimizer_grouped_parameters,
    lr=3e-5,
    eps=1e-8
)

from transformers import get_linear_schedule_with_warmup

epochs = 10 #100,200
max_grad_norm = 1.0

# Total number of training steps is number of batches * number of epochs.
total_steps = len(training_data_loader) * epochs

# Create the learning rate scheduler.
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

no_o = tag_ls[0:-1]

no_o

"""labels = [  'B-HPI_pos', 'B-HPI_neg', 'I-HPI_neg', 'B-HPI_qualified',
       'B-Descriptor', 'I-Descriptor', 'B-HPI_irrelevant', 'I-HPI_pos',
       'I-HPI_qualified', 'I-HPI_irrelevant', 'B-Molecular_neg',
       'B-Molecular_pos', '', 'I-Molecular_pos', 'I-Molecular_neg',
       'B-Body Part, Organ, or Organ Component_pos',
       'I-Iody Part, Organ, or Organ Component_pos',
       'B-Neoplastic Process', 'I-Neoplastic Process', 'B-Modifier',
       'I-Modifier', 'B-Neoplastic Process_pos',
       'I-Neoplastic Process_pos', 'B-Diagnostic Procedure_pos',
       'I-Diagnostic Procedure_pos']

Weighted Average
"""

## Store the average loss after each epoch so we can plot them.

import copy
loss_values, validation_loss_values, f1_val = [], [],[]
from sklearn.metrics import f1_score, classification_report
best_model_state = None
best_score = 1
for _ in trange(epochs, desc="Epoch"):
    # ========================================
    #               Training
    # ========================================
    # Perform one full pass over the training set.

    # Put the model into training mode.
    model.train()


    # Reset the total loss for this epoch.
    total_loss = 0

    # Training loop
    for step, batch in enumerate(training_data_loader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        # Always clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()
        # forward pass
        # This will return the loss (rather than the model output)
        # because we have provided the `labels`.
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask, labels=b_labels)
        # get the loss
        loss = outputs[0]
        # Perform a backward pass to calculate the gradients.
        loss.backward()
        # track train loss
        total_loss += loss.item()
        # Clip the norm of the gradient
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        # Update the learning rate.
        scheduler.step()
        

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(training_data_loader)
    print("Average train loss: {}".format(avg_train_loss))

    

    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)


    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    # Put the model into evaluation mode
    model.eval()
    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in validation_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)

    eval_loss = eval_loss / len(validation_data_loader) 
    validation_loss_values.append(eval_loss)
    print("Validation loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]
   
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]#[PAD]                             
    valid_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]#[PAD]
    f1_val.append(f1_score(pred_tags, valid_tags, average = 'macro',labels = tag_ls))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    try:
      print("Validation Accuracy: {}".format(accuracy_score(pred_tags, valid_tags)))
      #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
      #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
      print("Validation macro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'macro', labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
      print('Classification Report: {}'.format(classification_report(pred_tags, valid_tags,labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    

     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=tag_ls, zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=['B-HPI_pos','B-HPI_neg'], zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',

      print()

      print()
      mistakes = []
      predicted_tags = []
      true_tags = []
      for p, v in zip(pred_tags, valid_tags):
        predicted_tags.append(p)
        true_tags.append(v)
        if p != v:
          mistakes.append(p)
          mistakes.append(v)
          
      #print(f'predicted tags:{pred_tags}')
      #print(f'true tags:{valid_tags}')
      #print(f"mistakes list:{mistakes}")
      print(f'number of mistakes:{int(len(mistakes)/2)}')
      print('\n')

      if eval_loss < best_score and eval_loss > avg_train_loss:
          best_score = eval_loss
          best_model_state = copy.deepcopy(model.state_dict())
          print('current best evaluation loss is {0:.3f}'.format(best_score))
          print('\n')
    except:
      continue
print(f"final lowest evaluation loss is {best_score}")#excluding O class
    


 #if token is predicted incorectly, print token and surrounding tokens

## Store the average loss after each epoch so we can plot them.
'''
import copy
loss_values, validation_loss_values, f1_val = [], [],[]
from sklearn.metrics import f1_score, classification_report
best_model_state = None
best_score = -1
for _ in trange(epochs, desc="Epoch"):
    # ========================================
    #               Training
    # ========================================
    # Perform one full pass over the training set.

    # Put the model into training mode.
    model.train()


    # Reset the total loss for this epoch.
    total_loss = 0

    # Training loop
    for step, batch in enumerate(training_data_loader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        # Always clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()
        # forward pass
        # This will return the loss (rather than the model output)
        # because we have provided the `labels`.
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask, labels=b_labels)
        # get the loss
        loss = outputs[0]
        # Perform a backward pass to calculate the gradients.
        loss.backward()
        # track train loss
        total_loss += loss.item()
        # Clip the norm of the gradient
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        # Update the learning rate.
        scheduler.step()
        

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(training_data_loader)
    print("Average train loss: {}".format(avg_train_loss))

    

    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)


    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    # Put the model into evaluation mode
    model.eval()
    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in validation_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)

    eval_loss = eval_loss / len(validation_data_loader) 
    validation_loss_values.append(eval_loss)
    print("Validation loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]
   
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]#[PAD]                             
    valid_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]#[PAD]
    f1_val.append(f1_score(pred_tags, valid_tags, average = 'macro',labels = no_o))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    try:
      print("Validation Accuracy: {}".format(accuracy_score(pred_tags, valid_tags)))
      #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
      #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
      print("Validation macro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'macro', labels=no_o, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
      print('Classification Report: {}'.format(classification_report(pred_tags, valid_tags,labels=no_o, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    

      ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=['B-HPI_pos','B-HPI_neg'], zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      print()

      print()
      mistakes = []
      predicted_tags = []
      true_tags = []
      for p, v in zip(pred_tags, valid_tags):
        predicted_tags.append(p)
        true_tags.append(v)
        if p != v:
          mistakes.append(p)
          mistakes.append(v)
          
      #print(f'predicted tags:{pred_tags}')
      #print(f'true tags:{valid_tags}')
      #print(f"mistakes list:{mistakes}")
      print(f'number of mistakes:{int(len(mistakes)/2)}')
      print('\n')

      if best_score < ret:
          best_score = ret
          best_model_state = copy.deepcopy(model.state_dict())
          print('current best score is {0:.3f}'.format(best_score))
          print('\n')
    except:
      continue
print(f"final best F1 macro score is {best_score}")#excluding O class
    


 #if token is predicted incorectly, print token and surrounding tokens   
    '''

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

import seaborn as sns

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(loss_values, 'b-o', label="training loss")
plt.plot(validation_loss_values, 'r-o', label="validation loss")

# Label the plot.
plt.title("Learning curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.savefig('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/blue_loss_evloss.png')
plt.show()

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(f1_val, 'g-o', label="validation f1")
# Label the plot.
plt.title("Learning curve")
plt.xlabel("Epoch")
plt.ylabel("F1 Weigthed Average Score")
plt.legend()
plt.savefig('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/blue_f_score_evloss.png')
plt.show()

"""Save the model"""

#add timestamp to file name to track the version of weights saved
t = time.localtime()
timestamp = time.strftime('%d_%b_%H_%M', t)
model_file = ('blue_best_model_evloss'+timestamp+'.pt')
#path = f"./{model_file}"
path =f"/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/{model_file}"
#torch.save(model.state_dict(), path)
torch.save(best_model_state, path)

print(timestamp)

"""##Evaluate"""

from google.colab import drive
drive.mount('/content/drive')

model_file = ('blue_best_model_evloss'+timestamp+'.pt')
path =f"/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/{model_file}"

best_model = BertForTokenClassification.from_pretrained(
    #"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
    #"emilyalsentzer/Bio_ClinicalBERT",
    #"bert-base-uncased",
    'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12',
    #"dmis-lab/biobert-base-cased-v1.2",
    num_labels=len(tag2idx),
    output_attentions = False,
    output_hidden_states = False
)
best_model.to(device)
#best_model.load_state_dict(best_model_state)
best_model.load_state_dict(torch.load(path))

from sklearn.metrics import f1_score, classification_report
    loss_values, test_loss_values = [], []
    best_model.eval()
    # Reset the validation loss for this epoch.

    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels, tokens = [], [],[]
    for batch in test_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)

        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = best_model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        token_list = tokenizer.convert_ids_to_tokens(b_input_ids.to('cpu').numpy()[0])

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)
        tokens.extend(token_list)

    eval_loss = eval_loss / len(test_data_loader) 
    test_loss_values.append(eval_loss)


    print("Test loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]

    print(len(predictions))
    print(len(true_labels))
    print(predictions[0])
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[p_i] != "PAD"] 
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]                            
    test_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]
    print(len(pred_tags))
    print(len(test_tags))                              
    print("Test Accuracy: {}".format(accuracy_score(pred_tags, test_tags)))
    #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
    #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
    print("Test Macro F1-Score: {}".format(f1_score(pred_tags, test_tags, average = 'macro', labels =no_o)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    
    
    #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
    print('Classification Report: {}'.format(classification_report(pred_tags, test_tags, labels = no_o)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    #ret = (f1_score(pred_tags, test_tags, average = 'macro'))
    print()

    print()
    mistakes = []
    predicted_tags = []
    true_tags = []
    token_mistakes =[]
    
    #print(len(tokens))
    
    pred_tags = [tag_values[p_i] for p in predictions
                                  for p_i in p if tag_values[p_i] != "[PAD]"]   #[PAD]                          
    test_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"] #[PAD]
    for idx, (p, v) in enumerate(zip(pred_tags, test_tags)):
      predicted_tags.append(p)
      true_tags.append(v)
      if p != v :#and p != 'PAD':

        mistakes.append(p)
        mistakes.append(v)
        mistakes.append(tokens[idx-1])
        mistakes.append(tokens[idx])
        mistakes.append(tokens[idx+1])
        token_mistakes.append(tokens[idx])
    #print(f'predicted tags:{pred_tags}')
    #print(f'true tags:{valid_tags}')
    print(f"(predicted, true, before_mistake, token, after_mistake):{mistakes}")
    print(f'number of mistakes:{int(len(mistakes)/3)}')

    print(f'List of tokens predicted incorrectly:{token_mistakes}')

"""# IHC-Blue BERT

##Split Train/Test
"""

#tokenizer = BertTokenizerFast.from_pretrained("microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract", do_lower_case=False)
#tokenizer = BertTokenizerFast.from_pretrained("bert-base-cased", do_lower_case=False)

tokenizer = BertTokenizerFast.from_pretrained("/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/BLUE_IHC_BERT/weights/checkpoint-230000",do_lower_case=False)
#tokenizer = BertTokenizerFast.from_pretrained("emilyalsentzer/Bio_ClinicalBERT", do_lower_case=False)
#tokenizer = BertTokenizerFast.from_pretrained( "dmis-lab/biobert-base-cased-v1.2", do_lower_case=True)
#tokenizer = BertTokenizerFast.from_pretrained( "dmis-lab/biobert-base-cased-v1.2", do_lower_case=False)

def tokenize_and_preserve_labels(sentence, text_labels):
    tokenized_sentence = []
    labels = []

    for word, label in zip(sentence, text_labels):

        # Tokenize the word and count # of subwords the word is broken into
        tokenized_word = tokenizer.tokenize(word)
        n_subwords = len(tokenized_word)

        # Add the tokenized word to the final tokenized word list
        tokenized_sentence.extend(tokenized_word)

        # Add the same label to the new list of labels `n_subwords` times
        labels.extend([label] * n_subwords)

    return tokenized_sentence, labels

tokenized_texts_and_labels = [
    tokenize_and_preserve_labels(sent, labs)
    for sent, labs in zip(sentences, labels)
]

tokenized_texts_and_labels[0]

tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]
labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]


print(tokenized_texts)
print(labels)

data.Tag.unique()

#padding for sentences with maximum length 200
input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],
                          maxlen=max_len, dtype="long", value=0.0,
                          truncating="post", padding="post")
input_ids[2]

#padding for labels
tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],
                     maxlen=max_len, value=tag2idx["PAD"], padding="post",
                     dtype="long", truncating="post")


#tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],
 #                    maxlen=max_len, value=tag2idx["O"], padding="post",
  #                   dtype="long", truncating="post")

"""Loading data for training and testing.Creating the mask to ignore the padded elements in the sequences.

"""

#split train/val + test inputs
X_tr_val_inputs, X_test_inputs, y_tr_val_tags, y_test_tags = train_test_split(input_ids, tags,
                                                            random_state=9, test_size=0.2)
#split train + val 
X_tr_inputs, X_val_inputs, y_tr_tags, y_val_tags = train_test_split(X_tr_val_inputs, y_tr_val_tags,
                                                            random_state=9, test_size=0.1)
#split tr/val + test masks
attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]
tr_val_masks, test_masks, _, _ = train_test_split(attention_masks, input_ids,
                                             random_state=9, test_size=0.2)
#split tr + val masks
split_at_mask = [[float(i != 0.0) for i in ii] for ii in tr_val_masks]
tr_masks, val_masks, _, _ = train_test_split(split_at_mask, tr_val_masks,
                                             random_state=9, test_size=0.1)

#split into 3 train/val/test 
#split 80 train, and 20 test. Split 80 into 90 train and 10 val
#after each epoch select best epoch from validation and use the best for testing

tr_inputs = torch.tensor(X_tr_inputs)
val_inputs = torch.tensor(X_val_inputs)
test_inputs = torch.tensor(X_test_inputs)

tr_tags = torch.tensor(y_tr_tags)
val_tags = torch.tensor(y_val_tags)
test_tags = torch.tensor(y_test_tags)

tr_masks = torch.tensor(tr_masks)
val_masks = torch.tensor(val_masks)
test_masks = torch.tensor(test_masks)

# wrap tensors
train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)
valid_data = TensorDataset(val_inputs, val_masks, val_tags)
test_data = TensorDataset(test_inputs, test_masks, test_tags)

# samplers for sampling the data during training, validation and testing
sampler = RandomSampler(train_data)
val_sampler = RandomSampler(valid_data)
test_sampler = SequentialSampler(test_data)

# dataLoader for train, validation and test sets
training_data_loader = DataLoader(train_data, sampler=sampler, batch_size=bs)
validation_data_loader = DataLoader(valid_data, sampler=val_sampler, batch_size=bs)
test_data_loader = DataLoader(test_data, sampler=test_sampler)

"""##Train

BertForTokenClassification is a 
fine-tuning model that wraps BertModel and adds token-level classifier on top of the BertModel. The token-level classifier is a linear layer that takes as input the last hidden state of the sequence
"""

import transformers
from transformers import BertForTokenClassification, AdamW

transformers.__version__

len(tag2idx)

model = BertForTokenClassification.from_pretrained(
    #"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
    #"bert-base-uncased" ,
    #"emilyalsentzer/Bio_ClinicalBERT",
    #"dmis-lab/biobert-base-cased-v1.2",
    #'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12',
    '/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/BLUE_IHC_BERT/weights/checkpoint-230000',
    num_labels=len(tag2idx),
    output_attentions = True,
    output_hidden_states = True
)
model.cuda();

"""Select GPU if available"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

from transformers import AdamW
FULL_FINETUNING = True
if FULL_FINETUNING:
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'gamma', 'beta']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.0}
    ]
else:
    param_optimizer = list(model.classifier.named_parameters())
    optimizer_grouped_parameters = [{"params": [p for n, p in param_optimizer]}]

optimizer = AdamW(
    optimizer_grouped_parameters,
    lr=3e-5,
    eps=1e-8
)

from transformers import get_linear_schedule_with_warmup

epochs = 10 #100,200
max_grad_norm = 1.0

# Total number of training steps is number of batches * number of epochs.
total_steps = len(training_data_loader) * epochs

# Create the learning rate scheduler.
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

no_o = tag_ls[0:-1]

no_o

"""labels = [  'B-HPI_pos', 'B-HPI_neg', 'I-HPI_neg', 'B-HPI_qualified',
       'B-Descriptor', 'I-Descriptor', 'B-HPI_irrelevant', 'I-HPI_pos',
       'I-HPI_qualified', 'I-HPI_irrelevant', 'B-Molecular_neg',
       'B-Molecular_pos', '', 'I-Molecular_pos', 'I-Molecular_neg',
       'B-Body Part, Organ, or Organ Component_pos',
       'I-Iody Part, Organ, or Organ Component_pos',
       'B-Neoplastic Process', 'I-Neoplastic Process', 'B-Modifier',
       'I-Modifier', 'B-Neoplastic Process_pos',
       'I-Neoplastic Process_pos', 'B-Diagnostic Procedure_pos',
       'I-Diagnostic Procedure_pos']

Weighted Average
"""

## Store the average loss after each epoch so we can plot them.

import copy
loss_values, validation_loss_values, f1_val = [], [],[]
from sklearn.metrics import f1_score, classification_report
best_model_state = None
best_score = 1
for _ in trange(epochs, desc="Epoch"):
    # ========================================
    #               Training
    # ========================================
    # Perform one full pass over the training set.

    # Put the model into training mode.
    model.train()


    # Reset the total loss for this epoch.
    total_loss = 0

    # Training loop
    for step, batch in enumerate(training_data_loader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        # Always clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()
        # forward pass
        # This will return the loss (rather than the model output)
        # because we have provided the `labels`.
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask, labels=b_labels)
        # get the loss
        loss = outputs[0]
        # Perform a backward pass to calculate the gradients.
        loss.backward()
        # track train loss
        total_loss += loss.item()
        # Clip the norm of the gradient
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        # Update the learning rate.
        scheduler.step()
        

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(training_data_loader)
    print("Average train loss: {}".format(avg_train_loss))

    

    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)


    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    # Put the model into evaluation mode
    model.eval()
    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in validation_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)

    eval_loss = eval_loss / len(validation_data_loader) 
    validation_loss_values.append(eval_loss)
    print("Validation loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]
   
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]#[PAD]                             
    valid_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]#[PAD]
    f1_val.append(f1_score(pred_tags, valid_tags, average = 'macro',labels = tag_ls))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    try:
      print("Validation Accuracy: {}".format(accuracy_score(pred_tags, valid_tags)))
      #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
      #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
      print("Validation macro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'macro', labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
      print('Classification Report: {}'.format(classification_report(pred_tags, valid_tags,labels=tag_ls, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    

     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=tag_ls, zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
     # ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=['B-HPI_pos','B-HPI_neg'], zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',

      print()

      print()
      mistakes = []
      predicted_tags = []
      true_tags = []
      for p, v in zip(pred_tags, valid_tags):
        predicted_tags.append(p)
        true_tags.append(v)
        if p != v:
          mistakes.append(p)
          mistakes.append(v)
          
      #print(f'predicted tags:{pred_tags}')
      #print(f'true tags:{valid_tags}')
      #print(f"mistakes list:{mistakes}")
      print(f'number of mistakes:{int(len(mistakes)/2)}')
      print('\n')

      if eval_loss < best_score and eval_loss > avg_train_loss:
          best_score = eval_loss
          best_model_state = copy.deepcopy(model.state_dict())
          print('current best evaluation loss is {0:.3f}'.format(best_score))
          print('\n')
    except:
      continue
print(f"final lowest evaluation loss is {best_score}")#excluding O class
    


 #if token is predicted incorectly, print token and surrounding tokens

## Store the average loss after each epoch so we can plot them.

'''
import copy
loss_values, validation_loss_values, f1_val = [], [],[]
from sklearn.metrics import f1_score, classification_report
best_model_state = None
best_score = -1
for _ in trange(epochs, desc="Epoch"):
    # ========================================
    #               Training
    # ========================================
    # Perform one full pass over the training set.

    # Put the model into training mode.
    model.train()


    # Reset the total loss for this epoch.
    total_loss = 0

    # Training loop
    for step, batch in enumerate(training_data_loader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        # Always clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()
        # forward pass
        # This will return the loss (rather than the model output)
        # because we have provided the `labels`.
        outputs = model(b_input_ids, token_type_ids=None,
                        attention_mask=b_input_mask, labels=b_labels)
        # get the loss
        loss = outputs[0]
        # Perform a backward pass to calculate the gradients.
        loss.backward()
        # track train loss
        total_loss += loss.item()
        # Clip the norm of the gradient
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        # Update the learning rate.
        scheduler.step()
        

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(training_data_loader)
    print("Average train loss: {}".format(avg_train_loss))

    

    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)


    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    # Put the model into evaluation mode
    model.eval()
    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in validation_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)
        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)

    eval_loss = eval_loss / len(validation_data_loader) 
    validation_loss_values.append(eval_loss)
    print("Validation loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]
   
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]#[PAD]                             
    valid_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]#[PAD]
    f1_val.append(f1_score(pred_tags, valid_tags, average = 'macro',labels = no_o))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    try:
      print("Validation Accuracy: {}".format(accuracy_score(pred_tags, valid_tags)))
      #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
      #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
      print("Validation macro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'macro', labels=no_o, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
      print('Classification Report: {}'.format(classification_report(pred_tags, valid_tags,labels=no_o, zero_division=0)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    

      ret = (f1_score(pred_tags, valid_tags, average = 'macro',  labels=['B-HPI_pos','B-HPI_neg'], zero_division=0))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
      print()

      print()
      mistakes = []
      predicted_tags = []
      true_tags = []
      for p, v in zip(pred_tags, valid_tags):
        predicted_tags.append(p)
        true_tags.append(v)
        if p != v:
          mistakes.append(p)
          mistakes.append(v)
          
      #print(f'predicted tags:{pred_tags}')
      #print(f'true tags:{valid_tags}')
      #print(f"mistakes list:{mistakes}")
      print(f'number of mistakes:{int(len(mistakes)/2)}')
      print('\n')

      if best_score < ret:
          best_score = ret
          best_model_state = copy.deepcopy(model.state_dict())
          print('current best score is {0:.3f}'.format(best_score))
          print('\n')
    except:
      continue
print(f"final best F1 macro score is {best_score}")#excluding O class
    


 #if token is predicted incorectly, print token and surrounding tokens   
    '''

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

import seaborn as sns

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(loss_values, 'b-o', label="training loss")
plt.plot(validation_loss_values, 'r-o', label="validation loss")

# Label the plot.
plt.title("Learning curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.savefig('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/ihc_blue_loss_evloss.png')
plt.show()

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(f1_val, 'g-o', label="validation f1")
# Label the plot.
plt.title("Learning curve")
plt.xlabel("Epoch")
plt.ylabel("F1 Weigthed Average Score")
plt.legend()
plt.savefig('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/ihc_blue_f_score_evloss.png')
plt.show()

"""Save the model"""

#add timestamp to file name to track the version of weights saved
t = time.localtime()
timestamp = time.strftime('%d_%b_%H_%M', t)
model_file = ('ihc_blue_best_model_evloss'+timestamp+'.pt')
#path = f"./{model_file}"
path =f"/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/{model_file}"
#torch.save(model.state_dict(), path)
torch.save(best_model_state, path)

print(timestamp)

"""##Evaluate"""

model_file = ('ihc_blue_best_model_evloss'+timestamp+'.pt')
path =f"/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/{model_file}"

best_model = BertForTokenClassification.from_pretrained(
    #"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
    #"emilyalsentzer/Bio_ClinicalBERT",
    #"bert-base-uncased",
    '/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/BLUE_IHC_BERT/weights/checkpoint-230000',
    #"dmis-lab/biobert-base-cased-v1.2",
    num_labels=len(tag2idx),
    output_attentions = False,
    output_hidden_states = False
)
best_model.to(device)
#best_model.load_state_dict(best_model_state)
best_model.load_state_dict(torch.load(path))

from sklearn.metrics import f1_score, classification_report
    loss_values, test_loss_values = [], []
    best_model.eval()
    # Reset the validation loss for this epoch.

    # Reset the validation loss for this epoch.
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels, tokens = [], [],[]
    for batch in test_data_loader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        b_input_ids = b_input_ids.to(device)
        b_input_mask = b_input_mask.to(device)
        b_labels = b_labels.to(device)

        


        # Telling the model not to compute or store gradients,
        # saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have not provided labels.
            outputs = best_model(b_input_ids, token_type_ids=None,
                            attention_mask=b_input_mask, labels=b_labels)
        # Move logits and labels to CPU
        logits = outputs[1].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        token_list = tokenizer.convert_ids_to_tokens(b_input_ids.to('cpu').numpy()[0])

        # Calculate the accuracy for this batch of test sentences.
        eval_loss += outputs[0].mean().item()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.extend(label_ids)
        tokens.extend(token_list)

    eval_loss = eval_loss / len(test_data_loader) 
    test_loss_values.append(eval_loss)


    print("Test loss: {}".format(eval_loss))
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[l_i] != "PAD"]

    print(len(predictions))
    print(len(true_labels))
    print(predictions[0])
    #pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)
                                 #for p_i, l_i in zip(p, l) if tag_values[p_i] != "PAD"] 
    pred_tags = [tag_values[p_i] for p in predictions
                                 for p_i in p if tag_values[p_i] != "[PAD]"]                            
    test_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"]
    print(len(pred_tags))
    print(len(test_tags))                              
    print("Test Accuracy: {}".format(accuracy_score(pred_tags, test_tags)))
    #print("Validation F1-Score: {}".format(f1_score(listoflists(pred_tags), listoflists(valid_tags))))
    #print("Validation weighted F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'weighted')))
    print("Test weighted F1-Score: {}".format(f1_score(pred_tags, test_tags, average = 'weighted', labels =no_o)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    
    
    #print("Validation micro F1-Score: {}".format(f1_score(pred_tags, valid_tags, average = 'micro')))
    print('Classification Report: {}'.format(classification_report(pred_tags, test_tags, labels = no_o)))# 'B-Body Part, Organ, or Organ Component_pos','I-Iody Part, Organ, or Organ Component_pos',
    #ret = (f1_score(pred_tags, test_tags, average = 'macro'))
    print()

    print()
    mistakes = []
    predicted_tags = []
    true_tags = []
    token_mistakes =[]
    
    #print(len(tokens))
    
    pred_tags = [tag_values[p_i] for p in predictions
                                  for p_i in p if tag_values[p_i] != "[PAD]"]   #[PAD]                          
    test_tags = [tag_values[l_i] for l in true_labels
                                  for l_i in l if tag_values[l_i] != "[PAD]"] #[PAD]
    for idx, (p, v) in enumerate(zip(pred_tags, test_tags)):
      predicted_tags.append(p)
      true_tags.append(v)
      if p != v :#and p != 'PAD':

        mistakes.append(p)
        mistakes.append(v)
        mistakes.append(tokens[idx-1])
        mistakes.append(tokens[idx])
        mistakes.append(tokens[idx+1])
        token_mistakes.append(tokens[idx])
    #print(f'predicted tags:{pred_tags}')
    #print(f'true tags:{valid_tags}')
    print(f"(predicted, true, before_mistake, token, after_mistake):{mistakes}")
    print(f'number of mistakes:{int(len(mistakes)/3)}')

    print(f'List of tokens predicted incorrectly:{token_mistakes}')

test_sentence ='''
On immunohistochemistry the large cells express MONKEY, BAM-5, MM20 .'''

cols = ['Tag', 'Token']
exmpl = []
tokenized_sentence = tokenizer.encode(test_sentence)
input_ids = torch.tensor([tokenized_sentence]).cuda()
with torch.no_grad():
    output = best_model(input_ids)
label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
# join bpe split tokens
tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
new_tokens, new_labels = [], []
for token, label_idx in zip(tokens, label_indices[0]):
    if token.startswith("##"):
        new_tokens[-1] = new_tokens[-1] + token[2:]
    else:
        new_labels.append(tag_values[label_idx])
        new_tokens.append(token)
for token, label in zip(new_tokens, new_labels):
  exmpl.append([label,token])

  print("{}\t{}".format(label, token))
exmpl_df =  pd.DataFrame(exmpl, columns=cols)

"""# Test Sentences"""

test_sentence ='''

Sections show cores of fibroadipose tissue heavily infiltrated by nests and micropapillae of malignant epithelioid cells, some of which show glandular differentiation. The cells show marked nuclear pleomorphism, conspicuous nucleoli and brisk mitotic activity. No areas of necrosis are seen. The tumour cells stain positively for CK7, WT1, ER, p53 and PAX8, patchily positive for CA125 and negatively for CK20. The morphological and immunohistochemical features are those of a high grade serous adenocarcinoma of likely primary ovarian in origin. 
Omental core biopsy:  High grade serous adenocarcinoma of likely primary ovarian origin. 
 

'''

cols = ['Tag', 'Token']
exmpl = []
tokenized_sentence = tokenizer.encode(test_sentence)
input_ids = torch.tensor([tokenized_sentence]).cuda()
with torch.no_grad():
    output = best_model(input_ids)
label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
# join bpe split tokens
tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
new_tokens, new_labels = [], []
for token, label_idx in zip(tokens, label_indices[0]):
    if token.startswith("##"):
        new_tokens[-1] = new_tokens[-1] + token[2:]
    else:
        new_labels.append(tag_values[label_idx])
        new_tokens.append(token)
for token, label in zip(new_tokens, new_labels):
  exmpl.append([label,token])

  print("{}\t{}".format(label, token))
exmpl_df =  pd.DataFrame(exmpl, columns=cols)

exmpl_df.to_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/example.csv')

"""# CHOOSE BEST MODEL"""

# IHC-BIO_CLINICAL
tokenizer = BertTokenizerFast.from_pretrained("/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Clinical_IHC_BERT/weights/checkpoint-400000",do_lower_case=False)

test_data_group = data_group[data_group.index.isin(indices_test)].reset_index(drop=True)
test_data_group

'''test_inp = torch.tensor(X_test_inputs).cuda()
all_toks = []
for i in range(0, len(test_inp)-1):
 
  tokens = tokenizer.convert_ids_to_tokens(test_inp.to('cpu').numpy()[i])
  new_tokens = []
  for t in tokens:
    if t.startswith("##"):
          new_tokens[-1] = new_tokens[-1] + t[2:]
    elif t != '[PAD]':
      new_tokens.append(t)
  all_toks.append(new_tokens)

#get test dataset sentences
testsentdata = []
for i,r in data_group.iterrows():
  for s in all_toks:
  
    if ''.join(s) in ''.join(r['Word']):
      testsentdata.append(r['file_sent'])'''

"""# PREDICT TAGS FOR REPORTS"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device
from transformers import BertTokenizerFast, BertForTokenClassification
tokenizer = BertTokenizerFast.from_pretrained( "dmis-lab/biobert-base-cased-v1.2", do_lower_case=False)

import io

import pandas as pd
import numpy as np

"""with SEP and CLS"""

def predict_tags(text,true_tags,words,sent,marker):



  tokenized_sentence = tokenizer.encode(text, padding=True, truncation=True,max_length=512, add_special_tokens = True)
  input_ids = torch.tensor([tokenized_sentence]).cuda()

  with torch.no_grad():
      output = best_model(input_ids)
  label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
  # join bpe split tokens
  tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
  new_tokens, new_labels = [], []
  for token, label_idx in zip(tokens, label_indices[0]):
      if token.startswith("##"):
          new_tokens[-1] = new_tokens[-1] + token[2:]
      else:
          new_labels.append(tag_values[label_idx])
          new_tokens.append(token)
  pos = new_tokens.index('[CLS]')
  new_tokens.remove('[CLS]')
  del new_labels[pos]
  if '[SEP]' in new_tokens:
    pos = new_tokens.index('[SEP]')
    new_tokens.remove('[SEP]')
    del new_labels[pos]
    
  
  df = pd.DataFrame({'Tokens':new_tokens,'Labels':new_labels,'file_sent':sent})
  df1 = pd.DataFrame({'Words':words,'True_Tags':true_tags,'file_sent':sent,'marker':marker})

  #for tag, word,token, label in zip(true_tags,words,new_tokens, new_labels):
      #print("{}\t{}\t{}".format(tag, label, token))
      #if token != '[CLS]' :
   #     print(tag, word , label,token)
  #for tag, word in zip(true_tags,words):
        #print(tag,word)
  return df, df1

#add "if label != O get token. Then get unique..."  
df,df1 = predict_tags(data_group['file_sent'][12], data_group['Tag'][12], data_group['Word'][12],data_group['file_sent'][12
],data_group['text'][12
])
df1

"""without SEP and CLS"""

'''def predict_tags(text,true_tags,words,sent):



  tokenized_sentence = tokenizer.encode(text, padding=True, truncation=True,max_length=512, add_special_tokens = True)
  input_ids = torch.tensor([tokenized_sentence]).cuda()

  with torch.no_grad():
      output = best_model(input_ids)
  label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
  # join bpe split tokens
  tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
  new_tokens, new_labels = [], []
  for token, label_idx in zip(tokens, label_indices[0]):
      if token.startswith("##"):
          new_tokens[-1] = new_tokens[-1] + token[2:]
      else:
          new_labels.append(tag_values[label_idx])
          new_tokens.append(token)
  pos = new_tokens.index('[CLS]')
  new_tokens.remove('[CLS]')
  del new_labels[pos]
  if '[SEP]' in new_tokens:
    pos = new_tokens.index('[SEP]')
    new_tokens.remove('[SEP]')
    del new_labels[pos]
    
  
  df = pd.DataFrame({'Tokens':new_tokens,'Labels':new_labels,'Sentence#':sent})
  df1 = pd.DataFrame({'Words':words,'True_Tags':true_tags,'Sentence#':sent})

  #for tag, word,token, label in zip(true_tags,words,new_tokens, new_labels):
      #print("{}\t{}\t{}".format(tag, label, token))
      #if token != '[CLS]' :
   #     print(tag, word , label,token)
  #for tag, word in zip(true_tags,words):
        #print(tag,word)
  return df, df1

#add "if label != O get token. Then get unique..."  
df,df1 = predict_tags(data_group['Report'][35], data_group['Tag'][35], data_group['Word'][35],data_group['Sentence#'][35
])
df'''

#test_data_group= data_group.loc[data_group['Sentence#'].isin(testsentdata)].reset_index(drop= True)

test_data_group

'''#all data (including training reports)
all = pd.DataFrame(columns=['Tokens', 'Labels', 'file_sent'])
all_true = pd.DataFrame(columns=['Words', 'True_Tags', 'file_sent'])
n = 0
for i in range(0,len(data_group)):
  df,df1 = predict_tags(data_group['Report'][i], data_group['Tag'][i], data_group['Word'][i],data_group['file_sent'][i])
  all = pd.concat([all, df], ignore_index=True)
  all_true = pd.concat([all_true, df1], ignore_index=True)'''

#test data
all = pd.DataFrame(columns=['Tokens', 'Labels', 'file_sent'])
all_true = pd.DataFrame(columns=['Words', 'True_Tags', 'file_sent'])
n = 0
for i in range(0,len(test_data_group)):
  df,df1 = predict_tags(test_data_group['Report'][i], test_data_group['Tag'][i], test_data_group['Word'][i],test_data_group['file_sent'][i],test_data_group['text'][i])
  all = pd.concat([all, df], ignore_index=True)
  all_true = pd.concat([all_true, df1], ignore_index=True)

all_true

test_data_group["file1"] = test_data_group["file"].str[0]

test_data_group['file_sent']

len(test_data_group['file_sent'].unique())

"""### PRESENCES FOR GROUND TRUTH"""

len(all_true['file_sent'].unique())

data_group_true = all_true.groupby(
['file_sent'],as_index=False
)['Words', 'True_Tags','marker'].agg(lambda x: list(x))

data_group_true

markers = []
 import csv
 with open('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/true.csv', 'w') as f:
    
    for i in range (0, len(data_group_true)-1):
      for lab in data_group_true['True_Tags'][i]:
        if lab == 'B-HPI_pos' or lab == 'B-HPI_neg':
          pos = data_group_true['True_Tags'][i].index(lab)
          marker = data_group_true['marker'][i][pos]
          
          #row = "{}\t{}\t{}\n".format(marker, lab, data_group_true['file_sent'][i])
          #print(marker)
         
          #n=1
          #try:
            #if data_group_true['True_Tags'][i][pos+n] == 'I-HPI_pos' or data_group_true['True_Tags'][i][pos+n] == 'I-HPI_neg':
             # while data_group_true['True_Tags'][i][pos+n] == 'I-HPI_pos' or data_group_true['True_Tags'][i][pos+n] == 'I-HPI_neg':
              #  marker =  marker + data_group_true['Words'][i][pos+n]
               # n +=1
              
          #except:
           # continue
                
          
          row = "{}\t{}\t{}\n".format(marker, lab, data_group_true['file_sent'][i])
          if marker not in markers:
            markers.append(marker)
          #print(marker)
          f.write(row)

markers

'''markers = []
 import csv
 with open('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/true.csv', 'w') as f:
    
    for i in range (0, len(data_group_true)-1):
      for lab in data_group_true['True_Tags'][i]:
        if lab == 'B-HPI_pos' or lab == 'B-HPI_neg':
          pos = data_group_true['True_Tags'][i].index(lab)
          marker = data_group_true['Words'][i][pos]
          
          n=1
          if data_group_true['True_Tags'][i][pos+n] == 'I-HPI_pos' or data_group_true['True_Tags'][i][pos+n] == 'I-HPI_neg':
            while data_group_true['True_Tags'][i][pos+n] == 'I-HPI_pos' or data_group_true['True_Tags'][i][pos+n] == 'I-HPI_neg':
              marker =  marker + data_group_true['Words'][i][pos+n]

              n +=1
            
          row = "{}\t{}\t{}\n".format(marker, lab, data_group_true['file_sent'][i])
          f.write(row)'''

res = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/true.csv',sep='\t',names = ['Markers','Tags','file_sent'])
#res['Report#'] = res['Report#'].replace({'Sentence' : ''}, regex=True)

res['Markers']=res['Markers'].str.upper()

dictionary = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/ultimate_dictionary_final7.csv')
dictionary['Synonym']=dictionary['Synonym'].str.upper()
dictionary['use']=dictionary['use'].str.upper()
dictionary.head()

for i,r in res.iterrows():
  for ind, w in dictionary.iterrows():
    if (w['Synonym'] == r['Markers'] and r['Markers'] != w['use']) :#or (r['Markers'] in w['Synonym']  and r['Markers'] != w['use']) :
      #print(res['Markers'][i])
      res['Markers'][i] = dictionary['use'][ind]
      #print(res['Markers'][i])

mkrs = res.loc[((res['Tags']=='B-HPI_pos')|(res['Tags']=='B-HPI_neg'))]
mkrs

marker1 = list(mkrs['Markers'].unique())
names = []
names.append('file_sent')
for m in marker1:
  pos = str(m) + '-pos'
  neg = str(m) + '-neg'
  names.append(pos)
  names.append(neg)
  names.append(m)

marker1

print(f'number of markers is {int((len(marker1)))}')

res

data_group2_true= res.drop_duplicates().groupby(
['file_sent'],as_index=False
)['Markers', 'Tags'].agg(lambda x: list(x))
data_group2_true

presences_true = pd.DataFrame(columns=names)
for i, report in data_group2_true.iterrows():


  for m, l in zip(report['Markers'],report['Tags']):


      
      if l == 'B-HPI_pos':
          new = pd.DataFrame({'file_sent':[report['file_sent']],str(m) + '-pos':1,m :1})
          #new = pd.DataFrame({'file_sent':[report['file_sent']],str(m) :1})
      if l == 'B-HPI_neg':
          new = pd.DataFrame({'file_sent':[report['file_sent']],str(m) +'-neg':1,m :0})
          #new = pd.DataFrame({'file_sent':[report['file_sent']],str(m) :1})
      
         # print(m)
          
      presences_true = pd.concat([presences_true,new] , ignore_index=True)

presences_true=presences_true.groupby("file_sent").first().reset_index()

#all_true.rename(columns = {'Sentence#':'Report#'}, inplace = True)
#all_true['Report#'] = all_true['Report#'].replace({'Sentence' : ''}, regex=True)
all_true_reports = all_true['file_sent'].unique()
all_true_reports = pd.DataFrame({'file_sent':all_true_reports})

pr_all_rep_true = pd.merge(presences_true, all_true_reports, on ='file_sent', how ="outer")
pr_all_rep_true.fillna(2, inplace = True)
#pr_all_rep_true['file_sent']= pr_all_rep_true['file_sent']
pr_all_rep_true

#test
text = test_data_group[['Report','file_sent']]
#text.rename(columns = {'Sentence#':'Report#'}, inplace = True)
#text['Report#'] = text['Report#'].replace({'Sentence' : ''}, regex=True)
#text['Report#']=text['Report#'].astype(int)
text

'''text = data_group[['Report','Sentence#']]
text.rename(columns = {'Sentence#':'Report#'}, inplace = True)
text['Report#'] = text['Report#'].replace({'Sentence' : ''}, regex=True)
text['Report#']=text['Report#'].astype(int)'''

pr_all_rep_true_t = pd.merge( text,pr_all_rep_true, on ='file_sent', how ="outer")
#data_group_true.rename(columns = {'Sentence#':'Report#'}, inplace = True)
#data_group_true['Report#'] = data_group_true['Report#'].replace({'Sentence' : ''}, regex=True).astype(int)
pr_all_rep_true_fin = pd.merge(data_group_true, pr_all_rep_true_t, on ='file_sent', how ="outer")
pr_all_rep_true_fin[:50]

pr_all_rep_true_fin.to_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/pr_all_rep_true_descr_test_dataset.csv')

"""### PRESENCES FOR BIOBERT"""

all.columns

data_group1 = all.groupby(
['file_sent'],as_index=False
)['Tokens', 'Labels'].agg(lambda x: list(x))

markers = []
import csv
 with open('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/results.csv', 'w') as f:
    
    for i in range (0, len(data_group1)-1):
      for lab in data_group1['Labels'][i]:
        if lab == 'B-HPI_pos' or lab == 'B-HPI_neg':
          pos = data_group1['Labels'][i].index(lab)
          marker = data_group1['Tokens'][i][pos]
          try:
            n = 1
            
            if data_group1['Labels'][i][pos+n] == 'I-HPI_pos' or  data_group1['Labels'][i][pos+n] == 'I-HPI_neg':# data_group1['Labels'][i][pos+n] == 'B-HPI_pos' or data_group1['Labels'][i][pos+n] == 'B-HPI_neg' or
              while data_group1['Labels'][i][pos+n] == 'I-HPI_pos' or  data_group1['Labels'][i][pos+n] == 'I-HPI_neg':# data_group1['Labels'][i][pos+n] == 'B-HPI_pos' or data_group1['Labels'][i][pos+n] == 'B-HPI_neg' or
                marker =  marker + data_group1['Tokens'][i][pos+n]
              # print(marker)
                n +=1
          except:
            continue
          row = "{}\t{}\t{}\n".format(marker, lab, data_group1['file_sent'][i])
          f.write(row)

rest = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/results.csv',sep='\t',names = ['Markers','Labels','file_sent'])

rest['Markers']=rest['Markers'].str.upper()

len(rest['Markers'].unique())

'''res['Markers']=res['Markers'].str.upper()
for idx, r in res.iterrows():
  r['Markers'] = r['Markers'].replace('(+', '')
  r['Markers'] = r['Markers'].replace('(-', '')
  r['Markers'] = r['Markers'].replace('(+CD1A(+', '')
  r['Markers'] = r['Markers'].replace('-', '')
  r['Markers'] = r['Markers'].replace('(WEAK+', '')
  res['Markers'][idx] = r['Markers'].replace('-', '')'''

'''for i,r in res.iterrows():
  for ind, w in dictionary.iterrows():
    if (w['Synonym'] in r['Markers'] and r['Markers'] != w['use']):# or (r['Markers'] in w['Synonym']  and r['Markers'] != w['use']) :
      #print(res['Markers'][i])
      res['Markers'][i] = dictionary['use'][ind]
      #print(res['Markers'][i])'''

mkrs = rest.loc[((rest['Labels']=='B-HPI_pos')|(rest['Labels']=='B-HPI_neg'))]

marker = list(mkrs['Markers'].unique())

marker

names = []
names.append('file_sent')
for m in marker:
  names.append(m+'-pos')
  names.append(m+'-neg')
  names.append(m)

print(f'number of markers is {int(len(marker))}')



data_group2= rest.drop_duplicates().groupby(
['file_sent'],as_index=False
)['Markers', 'Labels'].agg(lambda x: list(x))
data_group2

presences = pd.DataFrame(columns=names)
for i, report in data_group2.iterrows():


  for m, l in zip(report['Markers'],report['Labels']):

     
      if l == 'B-HPI_pos':
          new = pd.DataFrame({'file_sent':[report['file_sent']],m + '-pos':1,str(m) :1})
         # print(m)
         # print(new)
      if l == 'B-HPI_neg':
          new = pd.DataFrame({'file_sent':[report['file_sent']],m +'-neg':1,str(m) :1})
         # print(m)
          
      presences = pd.concat([presences,new] , ignore_index=True)

presences=presences.groupby("file_sent").first().reset_index()

len(presences)

#all.rename(columns = {'Sentence#':'Report#'}, inplace = True)
#all['Report#'] = all['Report#'].replace({'Sentence' : ''}, regex=True)
all_reports = all['file_sent'].unique()
all_reports = pd.DataFrame({'file_sent':all_reports})

#all_reports['file_sent'] =all_reports['file_sent'].astype(int)
#presences['file_sent']=presences['file_sent'].astype(int)
pr_all_rep = pd.merge(presences, all_reports, on ='file_sent', how ="outer")
pr_all_rep.fillna(2, inplace = True)
pr_all_rep.head()

#presences['Report#']=presences['Report#'].astype(int)

pr_all_rep = pd.merge(presences, all_reports, on ='file_sent', how ="outer")
pr_all_rep.fillna(0, inplace = True)
pr_all_rep.head()

#test

text = test_data_group[['Report','file_sent']]
#text.rename(columns = {'Sentence#':'Report#'}, inplace = True)
#text['Report#'] = text['Report#'].replace({'Sentence' : ''}, regex=True)
#text['Report#']=text['Report#'].astype(int)

pr_all_rep_t = pd.merge( text,pr_all_rep, on ='file_sent', how ="outer")
#data_group1.rename(columns = {'Sentence#':'Report#'}, inplace = True)
#data_group1['Report#'] = data_group1['Report#'].replace({'Sentence' : ''}, regex=True).astype(int)
pr_all_rep_fin = pd.merge(data_group1, pr_all_rep_t, on ='file_sent', how ="outer")
pr_all_rep_fin

pr_all_rep_fin.to_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/pr_all_rep_bert_descr_test_dataset.csv')

"""CALCULATE HOW MANY REPORT DETECTED BY TRUE AND BERT"""

print(len(presences), 'sentences to have at least 1 marker were detected by BERT')
print(len(presences_true),'sentences to have at least 1 marker were in the ground truth')

for m, m1 in zip(marker, marker1):
  if m1 not in marker:
    print(f'Bert does not identify {m1}')
  if m not in marker1:
    print(f'Bert wrongly identifies {m} as a marker')

data_group1

data_group_true

"""### INCLUDE REPORTS WITHOUT MARKERs"""

all.head()

data_group1 = all.groupby(
['Sentence#'],as_index=False
)['Tokens', 'Labels'].agg(lambda x: list(x))
# Visualise data
data_group1.head()

df1[62:]

df1.head(30)

df, df1 =predict_tags(data_group['Report'][151], data_group['Tag'][151], data_group['Word'][151], data_group['Sentence#'][151])
df

df, df1 =predict_tags(data_group['Report'][151], data_group['Tag'][151], data_group['Word'][151])
df

df1[200:250]

test_sentence = '''
IHC showed CD20 (-), CD3 (+), CD3 to be positive, CK7 is equivocal '''
tokenized_sentence = tokenizer.encode(test_sentence)
input_ids = torch.tensor([tokenized_sentence]).cuda()
with torch.no_grad():
    output = best_model(input_ids)
label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
# join bpe split tokens
tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
new_tokens, new_labels = [], []
for token, label_idx in zip(tokens, label_indices[0]):
    if token.startswith("##"):
        new_tokens[-1] = new_tokens[-1] + token[2:]
    else:
        new_labels.append(tag_values[label_idx])
        new_tokens.append(token)
for token, label in zip(new_tokens, new_labels):
    print("{}\t{}".format(label, token))

test_sentence = '''
IHC showed CD20 was positive, but C-MYC was negative '''
tokenized_sentence = tokenizer.encode(test_sentence)
input_ids = torch.tensor([tokenized_sentence]).cuda()
with torch.no_grad():
    output = best_model(input_ids)
label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
# join bpe split tokens
tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
new_tokens, new_labels = [], []
for token, label_idx in zip(tokens, label_indices[0]):
    if token.startswith("##"):
        new_tokens[-1] = new_tokens[-1] + token[2:]
    else:
        new_labels.append(tag_values[label_idx])
        new_tokens.append(token)
for token, label in zip(new_tokens, new_labels):
    print("{}\t{}".format(label, token))

test_sentence = '''
CD-20 was positive, c-myc was not positive '''
tokenized_sentence = tokenizer.encode(test_sentence)
input_ids = torch.tensor([tokenized_sentence]).cuda()
with torch.no_grad():
    output = best_model(input_ids)
label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
# join bpe split tokens
tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
new_tokens, new_labels = [], []
for token, label_idx in zip(tokens, label_indices[0]):
    if token.startswith("##"):
        new_tokens[-1] = new_tokens[-1] + token[2:]
    else:
        new_labels.append(tag_values[label_idx])
        new_tokens.append(token)
for token, label in zip(new_tokens, new_labels):
    print("{}\t{}".format(label, token))

test_sentence = '''
IHC showed CD20 was positive, C-MYC was not expressed'''
tokenized_sentence = tokenizer.encode(test_sentence)
input_ids = torch.tensor([tokenized_sentence]).cuda()
with torch.no_grad():
    output = best_model(input_ids)
label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
# join bpe split tokens
tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
new_tokens, new_labels = [], []
for token, label_idx in zip(tokens, label_indices[0]):
    if token.startswith("##"):
        new_tokens[-1] = new_tokens[-1] + token[2:]
    else:
        new_labels.append(tag_values[label_idx])
        new_tokens.append(token)
for token, label in zip(new_tokens, new_labels):
    print("{}\t{}".format(label, token))

test_sentence = '''
IHC showed CD-3 was positive '''
tokenized_sentence = tokenizer.encode(test_sentence)
input_ids = torch.tensor([tokenized_sentence]).cuda()
with torch.no_grad():
    output = best_model(input_ids)
label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
# join bpe split tokens
tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
new_tokens, new_labels = [], []
for token, label_idx in zip(tokens, label_indices[0]):
    if token.startswith("##"):
        new_tokens[-1] = new_tokens[-1] + token[2:]
    else:
        new_labels.append(tag_values[label_idx])
        new_tokens.append(token)
for token, label in zip(new_tokens, new_labels):
    print("{}\t{}".format(label, token))

test_sentence = '''
CD-3 was negative '''
tokenized_sentence = tokenizer.encode(test_sentence)
input_ids = torch.tensor([tokenized_sentence]).cuda()
with torch.no_grad():
    output = best_model(input_ids)
label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
# join bpe split tokens
tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
new_tokens, new_labels = [], []
for token, label_idx in zip(tokens, label_indices[0]):
    if token.startswith("##"):
        new_tokens[-1] = new_tokens[-1] + token[2:]
    else:
        new_labels.append(tag_values[label_idx])
        new_tokens.append(token)
for token, label in zip(new_tokens, new_labels):
    print("{}\t{}".format(label, token))

"""# HODGKIN LYMPHOMA reports"""

hl= pd.read_excel('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/cHL_select_markers.xlsx')

hl

def predict_tags_hl(text,sent):



  tokenized_sentence = tokenizer.encode(text, padding=True, truncation=True,max_length=512, add_special_tokens = True)
  input_ids = torch.tensor([tokenized_sentence]).cuda()

  with torch.no_grad():
      output = best_model(input_ids)
  label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
  # join bpe split tokens
  tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
  new_tokens, new_labels = [], []
  for token, label_idx in zip(tokens, label_indices[0]):
      if token.startswith("##"):
          new_tokens[-1] = new_tokens[-1] + token[2:]
      else:
          new_labels.append(tag_values[label_idx])
          new_tokens.append(token)
  pos = new_tokens.index('[CLS]')
  new_tokens.remove('[CLS]')
  del new_labels[pos]
  if '[SEP]' in new_tokens:
    pos = new_tokens.index('[SEP]')
    new_tokens.remove('[SEP]')
    del new_labels[pos]
    
  
  df = pd.DataFrame({'Tokens':new_tokens,'Labels':new_labels,'specnum_formatted':sent})


  #for tag, word,token, label in zip(true_tags,words,new_tokens, new_labels):
      #print("{}\t{}\t{}".format(tag, label, token))
      #if token != '[CLS]' :
   #     print(tag, word , label,token)
  #for tag, word in zip(true_tags,words):
        #print(tag,word)
  return df

#add "if label != O get token. Then get unique..."  
df = predict_tags_hl(hl['full_text'][35], hl['specnum_formatted'][35
])
df[df['specnum_formatted']=='UH14-28279']

all = pd.DataFrame(columns=['Tokens', 'Labels', 'specnum_formatted'])

n = 0
for i in range(0,len(hl)):
  df = predict_tags_hl(hl['full_text'][i], hl['specnum_formatted'][i
])
  all = pd.concat([all, df], ignore_index=True)

all[all['specnum_formatted']=='UH14-28279'][50:100]

data_group1 = all.groupby(
['specnum_formatted'],as_index=False
)['Tokens', 'Labels'].agg(lambda x: list(x))

data_group1

markers = []
import csv
 with open('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/result_hl.csv', 'w') as f:
    
    for i in range (0, len(data_group1)-1):
      for lab in data_group1['Labels'][i]:
        if lab == 'B-HPI_pos' or lab == 'B-HPI_neg':
          pos = data_group1['Labels'][i].index(lab)
          marker = data_group1['Tokens'][i][pos]
          n = 1
          if data_group1['Labels'][i][pos+n] == 'I-HPI_pos' or  data_group1['Labels'][i][pos+n] == 'I-HPI_neg':# data_group1['Labels'][i][pos+n] == 'B-HPI_pos' or data_group1['Labels'][i][pos+n] == 'B-HPI_neg' or
            while data_group1['Labels'][i][pos+n] == 'I-HPI_pos' or  data_group1['Labels'][i][pos+n] == 'I-HPI_neg':#data_group1['Labels'][i][pos+n] == 'B-HPI_pos' or data_group1['Labels'][i][pos+n] == 'B-HPI_neg' or
              marker =  marker + data_group1['Tokens'][i][pos+n]
              print(marker)
              n +=1

          row = "{}\t{}\t{}\n".format(marker, lab, data_group1['specnum_formatted'][i])
          f.write(row)

hl_res = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/result_hl.csv',sep='\t',names = ['Markers','Labels','specnum_formatted'])
hl_res['Markers']= hl_res['Markers'].str.upper()

'''for i,r in hl_res.iterrows():
  for ind, w in dictionary.iterrows():
    if (w['Synonym'] in r['Markers'] and r['Markers'] != w['use']):# or (r['Markers'] in w['Synonym']  and r['Markers'] != w['use']) :

      hl_res['Markers'][i] = dictionary['use'][ind]
'''

mkrs = hl_res.loc[((hl_res['Labels']=='B-HPI_pos')|(hl_res['Labels']=='B-HPI_neg'))]
marker = list(mkrs['Markers'].unique())

names = []
names.append('specnum_formatted')
for m in marker:
  #names.append(m+'-pos')
  #names.append(m+'-neg')
  names.append(m)

print(f'number of markers is {len(marker)}')

marker

hl_res.drop_duplicates()

data_group2= hl_res.drop_duplicates().groupby(
['specnum_formatted'],as_index=False
)['Markers', 'Labels'].agg(lambda x: list(x))
data_group2

presences = pd.DataFrame(columns=names)
for i, report in data_group2.iterrows():


  for m, l in zip(report['Markers'],report['Labels']):
    
      if l == 'B-HPI_pos':
          new = pd.DataFrame({'specnum_formatted':[report['specnum_formatted']], m:1})
         # print(m)
         # print(new)
      elif l == 'B-HPI_neg':
          new = pd.DataFrame({'specnum_formatted':[report['specnum_formatted']],m:0})
         # print(m)
      
          
      presences = pd.concat([presences,new] , ignore_index=True)

presences=presences.groupby("specnum_formatted").first().reset_index()


all_reports = all['specnum_formatted'].unique()
all_reports = pd.DataFrame({'specnum_formatted':all_reports})

presences

pr_all_rep = pd.merge(presences, all_reports, on ='specnum_formatted', how ="outer")


#pr_all_rep['CD30'] = pr_all_rep['CD30'].fillna(pr_all_rep['CD30(+']) 
#pr_all_rep['PAX-5'] = pr_all_rep['PAX-5'].fillna(pr_all_rep['PAX-5(+']) 
#pr_all_rep['CD20'] = pr_all_rep['CD20'].fillna(pr_all_rep['CD20(-']) 
#pr_all_rep['CD20'] = pr_all_rep['CD20'].fillna(pr_all_rep['CD3,CD20']) 
#pr_all_rep['CD3'] = pr_all_rep['CD3'].fillna(pr_all_rep['CD3,CD20']) 
pr_all_rep.fillna(3, inplace = True)
pr_all_rep

pr_all_rep[pr_all_rep['CD15']==0]

text = hl[['full_text','specnum_formatted']]
pr_all_rep_t = pd.merge( text,pr_all_rep, on ='specnum_formatted', how ="outer")
pr_all_rep_fin = pd.merge(data_group1, pr_all_rep_t, on ='specnum_formatted', how ="outer")
pr_all_rep_fin

pr_all_rep_fin.to_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/pr_all_rep_bert_descr_hl.csv')



"""# HODGKIN LYMPHOMA reports"""

hl= pd.read_excel('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/Oment_Carcinoma.xlsx')

hl

def predict_tags_hl(text,sent):



  tokenized_sentence = tokenizer.encode(text, padding=True, truncation=True,max_length=512, add_special_tokens = True)
  input_ids = torch.tensor([tokenized_sentence]).cuda()

  with torch.no_grad():
      output = best_model(input_ids)
  label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
  # join bpe split tokens
  tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
  new_tokens, new_labels = [], []
  for token, label_idx in zip(tokens, label_indices[0]):
      if token.startswith("##"):
          new_tokens[-1] = new_tokens[-1] + token[2:]
      else:
          new_labels.append(tag_values[label_idx])
          new_tokens.append(token)
  pos = new_tokens.index('[CLS]')
  new_tokens.remove('[CLS]')
  del new_labels[pos]
  if '[SEP]' in new_tokens:
    pos = new_tokens.index('[SEP]')
    new_tokens.remove('[SEP]')
    del new_labels[pos]
    
  
  df = pd.DataFrame({'Tokens':new_tokens,'Labels':new_labels,'specnum_formatted':sent})


  #for tag, word,token, label in zip(true_tags,words,new_tokens, new_labels):
      #print("{}\t{}\t{}".format(tag, label, token))
      #if token != '[CLS]' :
   #     print(tag, word , label,token)
  #for tag, word in zip(true_tags,words):
        #print(tag,word)
  return df

#add "if label != O get token. Then get unique..."  
df = predict_tags_hl(hl['full_text'][35], hl['specnum_formatted'][35
])
df[df['specnum_formatted']=='UH14-28279']

all = pd.DataFrame(columns=['Tokens', 'Labels', 'specnum_formatted'])

n = 0
for i in range(0,len(hl)):
  df = predict_tags_hl(hl['full_text'][i], hl['specnum_formatted'][i
])
  all = pd.concat([all, df], ignore_index=True)

all[all['specnum_formatted']=='UH14-28279'][50:100]

data_group1 = all.groupby(
['specnum_formatted'],as_index=False
)['Tokens', 'Labels'].agg(lambda x: list(x))

data_group1

markers = []
import csv
 with open('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/result_hl.csv', 'w') as f:
    
    for i in range (0, len(data_group1)-1):
      for lab in data_group1['Labels'][i]:
        if lab == 'B-HPI_pos' or lab == 'B-HPI_neg':
          pos = data_group1['Labels'][i].index(lab)
          marker = data_group1['Tokens'][i][pos]
          n = 1
          if data_group1['Labels'][i][pos+n] == 'I-HPI_pos' or  data_group1['Labels'][i][pos+n] == 'I-HPI_neg':# data_group1['Labels'][i][pos+n] == 'B-HPI_pos' or data_group1['Labels'][i][pos+n] == 'B-HPI_neg' or
            while data_group1['Labels'][i][pos+n] == 'I-HPI_pos' or  data_group1['Labels'][i][pos+n] == 'I-HPI_neg':#data_group1['Labels'][i][pos+n] == 'B-HPI_pos' or data_group1['Labels'][i][pos+n] == 'B-HPI_neg' or
              marker =  marker + data_group1['Tokens'][i][pos+n]
              print(marker)
              n +=1

          row = "{}\t{}\t{}\n".format(marker, lab, data_group1['specnum_formatted'][i])
          f.write(row)

hl_res = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/result_hl.csv',sep='\t',names = ['Markers','Labels','specnum_formatted'])
hl_res['Markers']= hl_res['Markers'].str.upper()

'''for i,r in hl_res.iterrows():
  for ind, w in dictionary.iterrows():
    if (w['Synonym'] in r['Markers'] and r['Markers'] != w['use']):# or (r['Markers'] in w['Synonym']  and r['Markers'] != w['use']) :

      hl_res['Markers'][i] = dictionary['use'][ind]
'''

mkrs = hl_res.loc[((hl_res['Labels']=='B-HPI_pos')|(hl_res['Labels']=='B-HPI_neg'))]
marker = list(mkrs['Markers'].unique())

names = []
names.append('specnum_formatted')
for m in marker:
  #names.append(m+'-pos')
  #names.append(m+'-neg')
  names.append(m)

print(f'number of markers is {len(marker)}')

marker

hl_res.drop_duplicates()

data_group2= hl_res.drop_duplicates().groupby(
['specnum_formatted'],as_index=False
)['Markers', 'Labels'].agg(lambda x: list(x))
data_group2

presences = pd.DataFrame(columns=names)
for i, report in data_group2.iterrows():


  for m, l in zip(report['Markers'],report['Labels']):
    
      if l == 'B-HPI_pos':
          new = pd.DataFrame({'specnum_formatted':[report['specnum_formatted']], m:1})
         # print(m)
         # print(new)
      elif l == 'B-HPI_neg':
          new = pd.DataFrame({'specnum_formatted':[report['specnum_formatted']],m:0})
         # print(m)
      
          
      presences = pd.concat([presences,new] , ignore_index=True)

presences=presences.groupby("specnum_formatted").first().reset_index()


all_reports = all['specnum_formatted'].unique()
all_reports = pd.DataFrame({'specnum_formatted':all_reports})

presences

marker

pr_all_rep = pd.merge(presences, all_reports, on ='specnum_formatted', how ="outer")
pr_all_rep['WT1'] = pr_all_rep['WT1'].fillna(pr_all_rep['WT']) 
pr_all_rep['CK7'] = pr_all_rep['CK7'].fillna(pr_all_rep['CK']) 
pr_all_rep['CK7'] = pr_all_rep['CK7'].fillna(pr_all_rep['CYTOKERATIN7'])
pr_all_rep['PAX8'] = pr_all_rep['PAX8'].fillna(pr_all_rep['PAX'])

#pr_all_rep['CD30'] = pr_all_rep['CD30'].fillna(pr_all_rep['CD30(+']) 
#pr_all_rep['PAX-5'] = pr_all_rep['PAX-5'].fillna(pr_all_rep['PAX-5(+']) 
#pr_all_rep['CD20'] = pr_all_rep['CD20'].fillna(pr_all_rep['CD20(-']) 
#pr_all_rep['CD20'] = pr_all_rep['CD20'].fillna(pr_all_rep['CD3,CD20']) 
#pr_all_rep['CD3'] = pr_all_rep['CD3'].fillna(pr_all_rep['CD3,CD20']) 
pr_all_rep.fillna(3, inplace = True)
pr_all_rep

text = hl[['full_text','specnum_formatted']]
pr_all_rep_t = pd.merge( text,pr_all_rep, on ='specnum_formatted', how ="outer")
pr_all_rep_fin = pd.merge(data_group1, pr_all_rep_t, on ='specnum_formatted', how ="outer")
pr_all_rep_fin

pr_all_rep_fin.to_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/pr_all_rep_bert_descr_oc.csv')

"""# Ovarian Carcinoma reports"""

oc= pd.read_excel('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/Oment_Carcinoma.xlsx')

def predict_tags_hl(text,sent):



  tokenized_sentence = tokenizer.encode(text, padding=True, truncation=True,max_length=512, add_special_tokens = True)
  input_ids = torch.tensor([tokenized_sentence]).cuda()

  with torch.no_grad():
      output = best_model(input_ids)
  label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
  # join bpe split tokens
  tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
  new_tokens, new_labels = [], []
  for token, label_idx in zip(tokens, label_indices[0]):
      if token.startswith("##"):
          new_tokens[-1] = new_tokens[-1] + token[2:]
      else:
          new_labels.append(tag_values[label_idx])
          new_tokens.append(token)
  pos = new_tokens.index('[CLS]')
  new_tokens.remove('[CLS]')
  del new_labels[pos]
  if '[SEP]' in new_tokens:
    pos = new_tokens.index('[SEP]')
    new_tokens.remove('[SEP]')
    del new_labels[pos]
    
  
  df = pd.DataFrame({'Tokens':new_tokens,'Labels':new_labels,'specnum_formatted':sent})


  #for tag, word,token, label in zip(true_tags,words,new_tokens, new_labels):
      #print("{}\t{}\t{}".format(tag, label, token))
      #if token != '[CLS]' :
   #     print(tag, word , label,token)
  #for tag, word in zip(true_tags,words):
        #print(tag,word)
  return df

#add "if label != O get token. Then get unique..."  
df = predict_tags_hl(hl['full_text'][35], hl['specnum_formatted'][35
])
df[df['specnum_formatted']=='UH14-28279']

all = pd.DataFrame(columns=['Tokens', 'Labels', 'specnum_formatted'])

n = 0
for i in range(0,len(oc)):
  df = predict_tags_hl(oc['full_text'][i], oc['specnum_formatted'][i
])
  all = pd.concat([all, df], ignore_index=True)

data_group1 = all.groupby(
['specnum_formatted'],as_index=False
)['Tokens', 'Labels'].agg(lambda x: list(x))

data_group1

markers = []
import csv
 with open('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/result_oc.csv', 'w') as f:
    
    for i in range (0, len(data_group1)-1):
      for lab in data_group1['Labels'][i]:
        if lab == 'B-HPI_pos' or lab == 'B-HPI_neg':
          pos = data_group1['Labels'][i].index(lab)
          marker = data_group1['Tokens'][i][pos]
          n = 1
          if data_group1['Labels'][i][pos+n] == 'I-HPI_pos' or  data_group1['Labels'][i][pos+n] == 'I-HPI_neg':# data_group1['Labels'][i][pos+n] == 'B-HPI_pos' or data_group1['Labels'][i][pos+n] == 'B-HPI_neg' or
            while data_group1['Labels'][i][pos+n] == 'I-HPI_pos' or  data_group1['Labels'][i][pos+n] == 'I-HPI_neg':#data_group1['Labels'][i][pos+n] == 'B-HPI_pos' or data_group1['Labels'][i][pos+n] == 'B-HPI_neg' or
              marker =  marker + data_group1['Tokens'][i][pos+n]
              print(marker)
              n +=1

          row = "{}\t{}\t{}\n".format(marker, lab, data_group1['specnum_formatted'][i])
          f.write(row)

oc_res = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/result_oc.csv',sep='\t',names = ['Markers','Labels','specnum_formatted'])
oc_res['Markers']= oc_res['Markers'].str.upper()

'''for i,r in hl_res.iterrows():
  for ind, w in dictionary.iterrows():
    if (w['Synonym'] in r['Markers'] and r['Markers'] != w['use']):# or (r['Markers'] in w['Synonym']  and r['Markers'] != w['use']) :

      hl_res['Markers'][i] = dictionary['use'][ind]
'''

mkrs = oc_res.loc[((oc_res['Labels']=='B-HPI_pos')|(oc_res['Labels']=='B-HPI_neg'))]
marker = list(mkrs['Markers'].unique())

names = []
names.append('specnum_formatted')
for m in marker:
  #names.append(m+'-pos')
  #names.append(m+'-neg')
  names.append(m)

print(f'number of markers is {len(marker)}')

oc_res[oc_res['Markers']=='CK']

data_group2= oc_res.drop_duplicates().groupby(
['specnum_formatted'],as_index=False
)['Markers', 'Labels'].agg(lambda x: list(x))
data_group2

presences = pd.DataFrame(columns=names)
for i, report in data_group2.iterrows():


  for m, l in zip(report['Markers'],report['Labels']):
    
      if l == 'B-HPI_pos':
          new = pd.DataFrame({'specnum_formatted':[report['specnum_formatted']], m:1})
         # print(m)
         # print(new)
      elif l == 'B-HPI_neg':
          new = pd.DataFrame({'specnum_formatted':[report['specnum_formatted']],m:0})
         # print(m)
      
          
      presences = pd.concat([presences,new] , ignore_index=True)

#presences=presences.groupby("specnum_formatted").first().reset_index()


all_reports = all['specnum_formatted'].unique()
all_reports = pd.DataFrame({'specnum_formatted':all_reports})

marker

pr_all_rep = pd.merge(presences, all_reports, on ='specnum_formatted', how ="outer")
pr_all_rep['WT1'] = pr_all_rep['WT1'].fillna(pr_all_rep['WT']) 
pr_all_rep['CK7'] = pr_all_rep['CK7'].fillna(pr_all_rep['CK']) 
pr_all_rep['CK7'] = pr_all_rep['CK7'].fillna(pr_all_rep['CYTOKERATIN7'])
pr_all_rep['PAX8'] = pr_all_rep['PAX8'].fillna(pr_all_rep['PAX'])
#pr_all_rep['CD30'] = pr_all_rep['CD30'].fillna(pr_all_rep['CD30(+']) 
#pr_all_rep['PAX-5'] = pr_all_rep['PAX-5'].fillna(pr_all_rep['PAX-5(+']) 
#pr_all_rep['CD20'] = pr_all_rep['CD20'].fillna(pr_all_rep['CD20(-']) 
#pr_all_rep['CD20'] = pr_all_rep['CD20'].fillna(pr_all_rep['CD3,CD20']) 
#pr_all_rep['CD3'] = pr_all_rep['CD3'].fillna(pr_all_rep['CD3,CD20']) 
pr_all_rep.fillna(3, inplace = True)
pr_all_rep

text = oc[['full_text','specnum_formatted']]
pr_all_rep_t = pd.merge( text,pr_all_rep, on ='specnum_formatted', how ="outer")
pr_all_rep_fin = pd.merge(data_group1, pr_all_rep_t, on ='specnum_formatted', how ="outer")
pr_all_rep_fin.head()

pr_all_rep_fin.to_csv('/content/drive/MyDrive/Colab_Notebooks/MSc_BERT/IHC_BERT/Fine-tuning/pr_all_rep_bert_descr_oc.csv')